{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring Data Science Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll be discussing how to refactor code from data science projects to make them more robust, maintainable and scalable. \n",
    "Many data scientists today are trained in mathematics includin probability and statistics as well as other disciplines, however, not all data scientists have training as software engineers. Because of this lack of familiarity with proper software engineering principles,  Code in many data science projects is often underperformant, as well as difficult to maintain and scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this seminar, we will look at a simple data science project that trains a neural network on image recognition.  It will use a very common training set called *MNIST*.  MNIST is a set of gray scale hand drawn numbers.  The purpose of the project is to accurately distinguish different numbers from the images.  The challenge comes from the fact that many of the numbers in the images can be ambiguous.  A \"1\" may look like a \"7\", or a \"5\" may look like a \"6\".   The goal of the project is to look at the data set and accurately predict what is the correct number being displayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at the project code.  Our first file is main.py.  \n",
    "main.py starts by importing required libraries.\n",
    "\n",
    "It then sets the neural networks hyperparameters.\n",
    "Next it loads the testing and training data and decides which machine learning model to use. \n",
    "It sets a number of other parameters including the optimization function and the loss function.\n",
    "\n",
    "The code then runs the training and testing epochs for the model.\n",
    "It then calculates the training metrics and then at the end, it resets the project.\n",
    "This project also uses tensorboard to monitor the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import get_train_dataloader, get_test_dataloader\n",
    "from src.metrics import Metric\n",
    "from src.models import LinearNet\n",
    "from src.tracking import TensorboardExperiment, Stage\n",
    "from src.utils import generate_tensorboard_experiment_directory\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    'EPOCHS': 20,\n",
    "    'LR': 5e-5,\n",
    "    'OPTIMIZER': 'Adam',\n",
    "    'BATCH_SIZE': 128\n",
    "}\n",
    "\n",
    "# Data\n",
    "train_loader = get_train_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "test_loader = get_test_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "\n",
    "# Model and Optimizer\n",
    "model = LinearNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams.get('LR'))\n",
    "\n",
    "# Objective (loss) function\n",
    "compute_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Metric Containers\n",
    "train_accuracy = Metric()\n",
    "test_accuracy = Metric()\n",
    "y_true_batches = []\n",
    "y_pred_batches = []\n",
    "\n",
    "# Experiment Trackers\n",
    "log_dir = generate_tensorboard_experiment_directory(root='./runs')\n",
    "experiment = TensorboardExperiment(log_dir=log_dir)\n",
    "\n",
    "# Batch Counters\n",
    "test_batch = 0\n",
    "train_batch = 0\n",
    "\n",
    "for epoch in range(hparams.get('EPOCHS')):\n",
    "    # Testing Loop\n",
    "    for x_test, y_test in tqdm(test_loader, desc='Validation Batches', ncols=80):\n",
    "        test_batch += 1\n",
    "        test_batch_size = x_test.shape[0]\n",
    "        test_pred = model(x_test)\n",
    "        loss = compute_loss(test_pred, y_test)\n",
    "\n",
    "        # Compute Batch Validation Metrics\n",
    "        y_test_np = y_test.detach().numpy()\n",
    "        y_test_pred_np = np.argmax(test_pred.detach().numpy(), axis=1)\n",
    "        batch_test_accuracy = accuracy_score(y_test_np, y_test_pred_np)\n",
    "        test_accuracy.update(batch_test_accuracy, test_batch_size)\n",
    "        experiment.set_stage(Stage.VAL)\n",
    "        experiment.add_batch_metric('accuracy', batch_test_accuracy, test_batch)\n",
    "        y_true_batches += [y_test_np]\n",
    "        y_pred_batches += [y_test_pred_np]\n",
    "\n",
    "    # Training Loop\n",
    "    for x_train, y_train in tqdm(train_loader, desc='Train Batches', ncols=80):\n",
    "        train_batch += 1\n",
    "        train_batch_size = x_train.shape[0]\n",
    "        train_pred = model(x_train)\n",
    "        loss = compute_loss(train_pred, y_train)\n",
    "\n",
    "        # Compute Batch Training Metrics\n",
    "        y_train_np = y_train.detach().numpy()\n",
    "        y_train_pred_np = np.argmax(train_pred.detach().numpy(), axis=1)\n",
    "        batch_train_accuracy = accuracy_score(y_train_np, y_train_pred_np)\n",
    "        train_accuracy.update(batch_train_accuracy, train_batch_size)\n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        experiment.add_batch_metric('accuracy', batch_train_accuracy, train_batch)\n",
    "\n",
    "        # Reverse-mode AutoDiff (backpropagation)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute Average Epoch Metrics\n",
    "    summary = ', '.join([\n",
    "        f\"[Epoch: {epoch + 1}/{hparams.get('EPOCHS')}]\",\n",
    "        f\"Test Accuracy: {test_accuracy.average: 0.4f}\",\n",
    "        f\"Train Accuracy: {train_accuracy.average: 0.4f}\",\n",
    "    ])\n",
    "    print('\\n' + summary + '\\n')\n",
    "\n",
    "    # Log Validation Epoch Metrics\n",
    "    experiment.set_stage(Stage.VAL)\n",
    "    experiment.add_epoch_metric('accuracy', test_accuracy.average, epoch)\n",
    "    experiment.add_epoch_confusion_matrix(y_true_batches, y_pred_batches, epoch)\n",
    "\n",
    "    # Log Validation Epoch Metrics\n",
    "    experiment.set_stage(Stage.TRAIN)\n",
    "    experiment.add_epoch_metric('accuracy', train_accuracy.average, epoch)\n",
    "\n",
    "    # Reset metrics\n",
    "    train_accuracy.reset()\n",
    "    test_accuracy.reset()\n",
    "\n",
    "experiment.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the code, other than the main.py file, is located in a src folder.  Let's start looking at these modules. \n",
    "The first module we'll examine is dataset.py.\n",
    "\n",
    "This module handles loading the MNIST data set into memory.  IT also has some preprocessing methods to normalize the data for use with the training model. Note that the preprocessing_x method uses a variable (self.x) to store intermediate results.  This is problematic from a proper softwware engineering standpoint.  This is because self.x isn't going to have the same value at different stages during the program execution.   This is an example of a method that needs refactoring. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src.load_data import load_train_labels, load_train_data, load_test_data, load_test_labels\n",
    "\n",
    "\n",
    "class MNIST(Dataset):\n",
    "    idx: int  # requested data index\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "    TRAIN_MAX = 255.0\n",
    "    TRAIN_NORMALIZED_MEAN = 0.1306604762738429\n",
    "    TRAIN_NORMALIZED_STDEV = 0.3081078038564622\n",
    "\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        if len(data) != len(targets):\n",
    "            raise ValueError('data and targets must be the same length. '\n",
    "                             f'{len(data)} != {len(targets)}')\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.get_x(idx)\n",
    "        y = self.get_y(idx)\n",
    "        return x, y\n",
    "\n",
    "    def get_x(self, idx: int):\n",
    "        self.idx = idx\n",
    "        self.preprocess_x()\n",
    "        return self.x\n",
    "\n",
    "    def preprocess_x(self):\n",
    "        self.x = self.data[self.idx].copy().astype(np.float64)\n",
    "        self.x /= self.TRAIN_MAX\n",
    "        self.x -= self.TRAIN_NORMALIZED_MEAN\n",
    "        self.x /= self.TRAIN_NORMALIZED_STDEV\n",
    "        self.x = self.x.astype(np.float32)\n",
    "        self.x = torch.from_numpy(self.x)\n",
    "        self.x = self.x.unsqueeze(0)\n",
    "\n",
    "    def get_y(self, idx: int):\n",
    "        self.idx = idx\n",
    "        self.preprocess_y()\n",
    "        return self.y\n",
    "\n",
    "    def preprocess_y(self):\n",
    "        self.y = self.targets[self.idx]\n",
    "        self.y = torch.tensor(self.y, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_train_dataloader(batch_size: int) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        dataset=MNIST(load_train_data(), load_train_labels()),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_test_dataloader(batch_size: int) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        dataset=MNIST(load_test_data(), load_test_labels()),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the load_data.py module.  This is the low level module that actually handles loading the data directly from the disk. This module reads the data from the disk, make sure that the data is in unsigned byte format, checks the size and dimensions of the data and returns an np.array of both the data and the labels.  Note here that the load_test_data() and load_train_data() methods are nearly identical.  This is a target for refactoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = (Path(__file__).parent / \"../data\").resolve()\n",
    "\n",
    "ALLOWED_TYPES = {\n",
    "    \"UNSIGNED_BYTE\": b\"\\x08\",\n",
    "    \"SIGNED_BYTE\": b\"\\x09\",\n",
    "    \"SHORT\": b\"\\x0B\",\n",
    "    \"INT\": b\"\\x0C\",\n",
    "    \"SINGLE\": b\"\\x0D\",\n",
    "    \"DOUBLE\": b\"\\x0E\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    with gzip.open(DATA_DIR / \"t10k-images-idx3-ubyte.gz\", \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 3\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_images == 10_000\n",
    "\n",
    "        (num_rows,) = struct.unpack(\">I\", fp.read(4))\n",
    "        (num_cols,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_rows == num_cols == 28\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images * num_rows * num_cols\n",
    "\n",
    "    data = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    data = data.reshape((num_images, num_rows, num_cols))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    with gzip.open(DATA_DIR / \"train-images-idx3-ubyte.gz\", \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 3\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_images == 60_000\n",
    "\n",
    "        (num_rows,) = struct.unpack(\">I\", fp.read(4))\n",
    "        (num_cols,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_rows == num_cols == 28\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images * num_rows * num_cols\n",
    "\n",
    "    data = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    data = data.reshape((num_images, num_rows, num_cols))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_test_labels():\n",
    "    with gzip.open(DATA_DIR / \"t10k-labels-idx1-ubyte.gz\", \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 1\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_images == 10_000\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images\n",
    "\n",
    "    data = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_train_labels():\n",
    "    with gzip.open(DATA_DIR / \"train-labels-idx1-ubyte.gz\", \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 1\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_images == 60_000\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images\n",
    "\n",
    "    data = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the metrics.py module.  This is has the responsibility of collecting metrics about the machine learning module, i.e. how well is it doing in predicting correct results.  Note that we're importing the Real number set from the Python numbers library, but we're using it interchangeably with float values in other variables.  This is a target for refactoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Real\n",
    "\n",
    "\n",
    "class Metric:\n",
    "    values: list[Real]\n",
    "    running_total: float\n",
    "    num_updates: float\n",
    "    average: float\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Metric(average={self.average:0.4f})\"\n",
    "\n",
    "    def update(self, value: Real, batch_size: int):\n",
    "        self.values.append(value)\n",
    "        self.running_total += value * batch_size\n",
    "        self.num_updates += batch_size\n",
    "        self.average = self.running_total / self.num_updates\n",
    "\n",
    "    def reset(self):\n",
    "        self.values: list[Real] = []\n",
    "        self.running_total: float = 0.0\n",
    "        self.num_updates: float = 0.0\n",
    "        self.average: float = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model.py module creates our ML model using Pytorch.  Again, notice the structure of the forward() method.  We're using the same variable to store intermediate results.  This is a target for refactoring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LinearNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(in_features=28 * 28, out_features=32)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(in_features=32, out_features=10)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tracking module is used to send the results to tensorboard.  Tensorboard is a web based application which displays metrics in a visual format. Note that the ExperimentTracker is an abstract base class, but not all methods in this class are abstract.  We'll see how we can replace the abstract base class with a new feature in Python called a *Protocol class*. \n",
    "Additionally, we'll discuss another new Python feature called a *Dataclass.*  Also note that we have a Stage class with three values  This might better be implemented as an Enum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from numbers import Real\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Stage:\n",
    "    TRAIN: str = 'train'\n",
    "    TEST: str = 'test'\n",
    "    VAL: str = 'val'\n",
    "\n",
    "\n",
    "class ExperimentTracker(ABC):\n",
    "    stage: str\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_batch_metric(self, name: str, value: Real, step: int):\n",
    "        \"\"\"Implements logging a batch-level metric.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_epoch_metric(self, name: str, value: Real, step: int):\n",
    "        \"\"\"Implements logging a epoch-level metric.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_epoch_confusion_matrix(self, y_true: np.array, y_pred: np.array, step: int):\n",
    "        \"\"\"Implements logging a confusion matrix at epoch-level.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_hparams(self, hparams: dict[str, Union[str, Real]], metrics: dict[str, Real]):\n",
    "        \"\"\"Implements logging hyperparameters.\"\"\"\n",
    "\n",
    "    def add_batch_metrics(self, values: dict[str, Real], step: int):\n",
    "        for name, value in values.items():\n",
    "            self.add_batch_metric(name, value, step)\n",
    "\n",
    "    def add_epoch_metrics(self, values: dict[str, Real], step: int):\n",
    "        for name, value in values.items():\n",
    "            self.add_epoch_metric(name, value, step)\n",
    "\n",
    "\n",
    "class TensorboardExperiment(ExperimentTracker):\n",
    "\n",
    "    def __init__(self, log_dir: str, create=True):\n",
    "        self._validate_log_dir(log_dir, create=create)\n",
    "        self._writer = SummaryWriter(log_dir=log_dir)\n",
    "        plt.ioff()\n",
    "\n",
    "    def set_stage(self, stage: str):\n",
    "        self.stage = stage\n",
    "        return self\n",
    "\n",
    "    def flush(self):\n",
    "        self._writer.flush()\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_log_dir(log_dir, create=True):\n",
    "        log_dir = Path(log_dir).resolve()\n",
    "        if log_dir.exists():\n",
    "            return\n",
    "        elif not log_dir.exists() and create:\n",
    "            log_dir.mkdir(parents=True)\n",
    "        else:\n",
    "            raise NotADirectoryError(f'log_dir {log_dir} does not exist.')\n",
    "\n",
    "    def add_batch_metric(self, name: str, value: Real, step: int):\n",
    "        tag = f'{self.stage}/batch/{name}'\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def add_epoch_metric(self, name: str, value: Real, step: int):\n",
    "        tag = f'{self.stage}/epoch/{name}'\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def add_epoch_confusion_matrix(self, y_true: list[np.array], y_pred: list[np.array], step: int):\n",
    "        y_true, y_pred = self.collapse_batches(y_true, y_pred)\n",
    "        fig = self.create_confusion_matrix(y_true, y_pred, step)\n",
    "        tag = f'{self.stage}/epoch/confusion_matrix'\n",
    "        self._writer.add_figure(tag, fig, step)\n",
    "\n",
    "    @staticmethod\n",
    "    def collapse_batches(y_true: list[np.array], y_pred: list[np.array]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return np.concatenate(y_true), np.concatenate(y_pred)\n",
    "\n",
    "    def create_confusion_matrix(self, y_true: np.array, y_pred: np.array, step: int) -> plt.Figure:\n",
    "        cm = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred)).plot(cmap='Blues')\n",
    "        fig: plt.Figure = cm.figure_\n",
    "        ax: plt.Axes = cm.ax_\n",
    "        ax.set_title(f'{self.stage.title()} Epoch: {step}')\n",
    "        return fig\n",
    "\n",
    "    def add_hparams(self, hparams: dict[str, Union[str, Real]], metrics: dict[str, Real]):\n",
    "        _metrics = self._validate_hparam_metric_keys(metrics)\n",
    "        self._writer.add_hparams(hparams, _metrics)\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_hparam_metric_keys(metrics):\n",
    "        _metrics = metrics.copy()\n",
    "        prefix = 'hparam/'\n",
    "        for name in _metrics.keys():\n",
    "            if not name.startswith(prefix):\n",
    "                _metrics[f'{prefix}{name}'] = _metrics[name]\n",
    "                del _metrics[name]\n",
    "        return _metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a file called *utils.py*.  Which, as the name suggests contain a number of utility functions used by other parts of the application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def generate_tensorboard_experiment_directory(root: str, parents=True) -> str:\n",
    "    root = Path(root).resolve()\n",
    "    child = create_from_missing(root) if not root.exists() else create_from_existing(root)\n",
    "    child.mkdir(parents=parents)\n",
    "    return child.as_posix()\n",
    "\n",
    "\n",
    "def create_from_missing(root):\n",
    "    return root / '0'\n",
    "\n",
    "\n",
    "def create_from_existing(root):\n",
    "    children = [int(c.name) for c in root.glob('*') if (c.is_dir() and c.name.isnumeric())]\n",
    "    if is_first_experiment(children):\n",
    "        child = root / '0'\n",
    "    else:\n",
    "        child = root / increment_experiment_number(children)\n",
    "    return child\n",
    "\n",
    "\n",
    "def is_first_experiment(children: list[int]) -> bool:\n",
    "    return len(children) == 0\n",
    "\n",
    "\n",
    "def increment_experiment_number(children: list[int]) -> str:\n",
    "    return str(max(children) + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Small Digression\n",
    "\n",
    "Here we'll digress from our topic to discuss two features of Python that we will use in our refactoring. \n",
    "The first features is the new Protocol class that was introduced in Python 3.8.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protocol Classes\n",
    "The closest resemblance to a Protocol class in other languages might be the *Interface* features in Java.  Protocol classes are used as an implicit base class for other classes. \n",
    "\n",
    "Any class that has the same methods defined as the Protocol class are determined to be subclasses of the base class for any static typing analysis. \n",
    "\n",
    "Here's a simple exxample of a Protocol class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations manager\n",
      "Line worker\n"
     ]
    }
   ],
   "source": [
    "from typing import Protocol\n",
    "\n",
    "class Person(Protocol):\n",
    "    \n",
    "    def returnRole(self) -> str:\n",
    "        pass\n",
    "    \n",
    "class Employee:\n",
    "    def __init__(self, role: str,salary: float):\n",
    "        self.role = role\n",
    "        self.salary = salary\n",
    "        \n",
    "    def returnRole(self) -> str:\n",
    "        return self.role\n",
    "    \n",
    "class Manager:\n",
    "    def __init__(self,role:str, salary: float):\n",
    "        self.role = role\n",
    "        self.salary = salary\n",
    "        \n",
    "    def returnRole(self) -> str:\n",
    "        return self.role\n",
    "    \n",
    "m = Manager('Operations manager',50000.00)\n",
    "e = Employee('Line worker',35000.00)\n",
    "\n",
    "print (m.returnRole())\n",
    "print (e.returnRole())\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't have to explicitly define an inheritance structure in our sub classes.  Python uses *duck typing* to figure out the inheritance structure for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Classes ###\n",
    "The data class feature in Python is designed to make it easy to write classes where the main purpose of the class is to store data, rather than implement logic via methods.   Here's an example of how we would use a data class.\n",
    "\n",
    "Let's take an example of a class called Person. Before data classes we might easily implement a Person object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person:\n",
    "    numbeer_of_people: int = 0\n",
    "    def __init__(self,name: str, age: int, address:str):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        self.address = address\n",
    "        \n",
    "    def happyBirthday(self):\n",
    "        self.age += 1\n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        return f\"Name: {self.name}e Age:  {self.age} Address {self.address}\" \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a class like Person would require creating an __init__() method, as well as creating either a __str__() method, or a __repr__() method, or both.  \n",
    "\n",
    "Here's how we would do the same thing using a dataclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person(name='Braun', age=21, address='1234 Main Street')\n",
      "Person(name='Jon', age=19, address='1235 Main Street')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    \n",
    "    name: str\n",
    "    age:  int\n",
    "    address: str\n",
    "    number_of_people: ClassVar[int] = 0\n",
    "        \n",
    "    def __post__init__(self):\n",
    "        print (type(number_of_people))\n",
    "      #  Person.number_of_people[0] += 1\n",
    "        \n",
    "p1 = Person(\"Braun\", 21, \"1234 Main Street\")\n",
    "p2 = Person(\"Jon\", 19, \"1235 Main Street\")\n",
    "print (p1)\n",
    "print (p2)\n",
    "print (p1.number_of_people)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataclass handles much of the boilerplate that you would be required to write yourself.  No longer do you need to create an \\_\\_init\\_\\_() method, nor do you need to override the \\_\\_str\\_\\_() method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's refactor\n",
    "Lets start by looking at the ExperimentTracker class.  Specificlly this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Stage:\n",
    "    TRAIN: str = 'train'\n",
    "    TEST: str = 'test'\n",
    "    VAL: str = 'val'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a dataclass called Stage(The frozen=True option makes Stage immutable).  A better way to do this is to turn this into a enumeration (enum).   So, our new code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto()\n",
    "\n",
    "class Stage(Enum):\n",
    "    \n",
    "    TRAIN = auto()\n",
    "    TEST = auto()\n",
    "    VAL = auto()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auto() method assigns an incrementing value to each element of the enum members. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to our Tracking class.  There are a number of things we want to change.  We've already looked at changing the stage type to an enum, but also note that this is defined in the abstract base class.  It is usually never a great idea to define concrete variable types inside an ABC, so we'll move this out of the ABC and into the concrete class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from numbers import Real\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple\n",
    "from enum import Enum,  auto()\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "class Stage(Enum):\n",
    "    TRAIN = auto()\n",
    "    TEST = auto()\n",
    "    VAL = auto()\n",
    "\n",
    "class ExperimentTracker(ABC):\n",
    "    \n",
    "    #Note here that stage is no longer a string, but a Stage enum, so we need to change its type. \n",
    "    # Additionally we want to remove the entire definition out of the Abstract class and into the\n",
    "    #concrete implementation. \n",
    "    #stage: str\n",
    "    \n",
    "\n",
    "    @abstractmethod\n",
    "    def add_batch_metric(self, name: str, value: Real, step: int):\n",
    "        \"\"\"Implements logging a batch-level metric.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_epoch_metric(self, name: str, value: Real, step: int):\n",
    "        \"\"\"Implements logging a epoch-level metric.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_epoch_confusion_matrix(self, y_true: np.array, y_pred: np.array, step: int):\n",
    "        \"\"\"Implements logging a confusion matrix at epoch-level.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_hparams(self, hparams: dict[str, Union[str, Real]], metrics: dict[str, Real]):\n",
    "        \"\"\"Implements logging hyperparameters.\"\"\"\n",
    "\n",
    "    def add_batch_metrics(self, values: dict[str, Real], step: int):\n",
    "        for name, value in values.items():\n",
    "            self.add_batch_metric(name, value, step)\n",
    "\n",
    "    def add_epoch_metrics(self, values: dict[str, Real], step: int):\n",
    "        for name, value in values.items():\n",
    "            self.add_epoch_metric(name, value, step)\n",
    "\n",
    "\n",
    "class TensorboardExperiment(ExperimentTracker):\n",
    "  \n",
    "# Move this from Abstract to concrete implementation. \n",
    "    stage: Stage\n",
    "\n",
    "    def __init__(self, log_dir: str, create=True):\n",
    "        self._validate_log_dir(log_dir, create=create)\n",
    "        self._writer = SummaryWriter(log_dir=log_dir)\n",
    "        plt.ioff()\n",
    "\n",
    "        #Change the stage type here as well. \n",
    "    def set_stage(self, stage: Stage):\n",
    "        self.stage = stage\n",
    "        return self\n",
    "\n",
    "    def flush(self):\n",
    "        self._writer.flush()\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_log_dir(log_dir, create=True):\n",
    "        log_dir = Path(log_dir).resolve()\n",
    "        if log_dir.exists():\n",
    "            return\n",
    "        elif not log_dir.exists() and create:\n",
    "            log_dir.mkdir(parents=True)\n",
    "        else:\n",
    "            raise NotADirectoryError(f'log_dir {log_dir} does not exist.')\n",
    "      #Now that we've set the stage as an enum, we want to print out the stage name, not its value. \n",
    "      # We do this by chaining the name() method to the stage variable.\n",
    "    def add_batch_metric(self, name: str, value: Real, step: int):\n",
    "       # tag = f'{self.stage}/batch/{name}'\n",
    "        tag = f'{self.stage.name}/batch/{name}'\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "        \n",
    "    def add_epoch_metric(self, name: str, value: Real, step: int):\n",
    "        \n",
    "   \n",
    "      # Again, add the .name attribute. \n",
    "      #  tag = f'{self.stage}/epoch/{name}'\n",
    "        tag = f'{self.stage.name}/epoch/{name}'\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def add_epoch_confusion_matrix(self, y_true: list[np.array], y_pred: list[np.array], step: int):\n",
    "        y_true, y_pred = self.collapse_batches(y_true, y_pred)\n",
    "        fig = self.create_confusion_matrix(y_true, y_pred, step)\n",
    "        # Add the name attribute to the stage variable here as well. \n",
    "        # tag = f'{self.stage}/epoch/confusion_matrix'\n",
    "        tag = f'{self.stage.name}/epoch/confusion_matrix'\n",
    "\n",
    "        self._writer.add_figure(tag, fig, step)\n",
    "\n",
    "    @staticmethod\n",
    "    def collapse_batches(y_true: list[np.array], y_pred: list[np.array]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return np.concatenate(y_true), np.concatenate(y_pred)\n",
    "\n",
    "    def create_confusion_matrix(self, y_true: np.array, y_pred: np.array, step: int) -> plt.Figure:\n",
    "        cm = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred)).plot(cmap='Blues')\n",
    "        fig: plt.Figure = cm.figure_\n",
    "        ax: plt.Axes = cm.ax_\n",
    "        ax.set_title(f'{self.stage.title()} Epoch: {step}')\n",
    "        return fig\n",
    "\n",
    "    def add_hparams(self, hparams: dict[str, Union[str, Real]], metrics: dict[str, Real]):\n",
    "        _metrics = self._validate_hparam_metric_keys(metrics)\n",
    "        self._writer.add_hparams(hparams, _metrics)\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_hparam_metric_keys(metrics):\n",
    "        _metrics = metrics.copy()\n",
    "        prefix = 'hparam/'\n",
    "        for name in _metrics.keys():\n",
    "            if not name.startswith(prefix):\n",
    "                _metrics[f'{prefix}{name}'] = _metrics[name]\n",
    "                del _metrics[name]\n",
    "        return _metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now pay attention to the ExperimentTracker abstract base class.  We note that this class consists of abstract and non-abstract methods.  Ideally, we'd like to move all concrete implementations out of the ABC and into the concrete class.  However, we notice that in this case, none of those non-abstract methods are actually being used. They really just call the abstract methods.   So, let's get rid of them altogether.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from numbers import Real\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple\n",
    "from enum import Enum,  auto()\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Stage(Enum):\n",
    "    TRAIN = auto()\n",
    "    TEST = auto()\n",
    "    VAL = auto()\n",
    "\n",
    "class ExperimentTracker(ABC):\n",
    "    \n",
    "    #Note here that stage is no longer a string, but a Stage enum, so we need to change its type. \n",
    "    # Additionally we want to remove the entire definition out of the Abstract class and into the\n",
    "    #concrete implementation. \n",
    "    #stage: str\n",
    "    \n",
    "\n",
    "    @abstractmethod\n",
    "    def add_batch_metric(self, name: str, value: Real, step: int):\n",
    "        \"\"\"Implements logging a batch-level metric.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_epoch_metric(self, name: str, value: Real, step: int):\n",
    "        \"\"\"Implements logging a epoch-level metric.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_epoch_confusion_matrix(self, y_true: np.array, y_pred: np.array, step: int):\n",
    "        \"\"\"Implements logging a confusion matrix at epoch-level.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def add_hparams(self, hparams: dict[str, Union[str, Real]], metrics: dict[str, Real]):\n",
    "        \"\"\"Implements logging hyperparameters.\"\"\"\n",
    "\n",
    "## Removed the non-abstract methods from the ABC as they're never used. \n",
    "\n",
    "class TensorboardExperiment(ExperimentTracker):\n",
    "  \n",
    "# Move this from Abstract to concrete implementation. \n",
    "    stage: Stage\n",
    "\n",
    "    def __init__(self, log_dir: str, create=True):\n",
    "        self._validate_log_dir(log_dir, create=create)\n",
    "        self._writer = SummaryWriter(log_dir=log_dir)\n",
    "        plt.ioff()\n",
    "\n",
    "        #Change the stage type here as well. \n",
    "    def set_stage(self, stage: Stage):\n",
    "        self.stage = stage\n",
    "        return self\n",
    "\n",
    "    def flush(self):\n",
    "        self._writer.flush()\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_log_dir(log_dir, create=True):\n",
    "        log_dir = Path(log_dir).resolve()\n",
    "        if log_dir.exists():\n",
    "            return\n",
    "        elif not log_dir.exists() and create:\n",
    "            log_dir.mkdir(parents=True)\n",
    "        else:\n",
    "            raise NotADirectoryError(f'log_dir {log_dir} does not exist.')\n",
    "      #Now that we've set the stage as an enum, we want to print out the stage name, not its value. \n",
    "      # We do this by chaining the name() method to the stage variable.\n",
    "    def add_batch_metric(self, name: str, value: Real, step: int):\n",
    "       # tag = f'{self.stage}/batch/{name}'\n",
    "        tag = f'{self.stage.name}/batch/{name}'\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "        \n",
    "    def add_epoch_metric(self, name: str, value: Real, step: int):\n",
    "        \n",
    "   \n",
    "      # Again, add the .name attribute. \n",
    "      #  tag = f'{self.stage}/epoch/{name}'\n",
    "        tag = f'{self.stage.name}/epoch/{name}'\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def add_epoch_confusion_matrix(self, y_true: list[np.array], y_pred: list[np.array], step: int):\n",
    "        y_true, y_pred = self.collapse_batches(y_true, y_pred)\n",
    "        fig = self.create_confusion_matrix(y_true, y_pred, step)\n",
    "        # Add the name attribute to the stage variable here as well. \n",
    "        # tag = f'{self.stage}/epoch/confusion_matrix'\n",
    "        tag = f'{self.stage.name}/epoch/confusion_matrix'\n",
    "\n",
    "        self._writer.add_figure(tag, fig, step)\n",
    "\n",
    "    @staticmethod\n",
    "    def collapse_batches(y_true: list[np.array], y_pred: list[np.array]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return np.concatenate(y_true), np.concatenate(y_pred)\n",
    "\n",
    "    def create_confusion_matrix(self, y_true: np.array, y_pred: np.array, step: int) -> plt.Figure:\n",
    "        cm = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred)).plot(cmap='Blues')\n",
    "        fig: plt.Figure = cm.figure_\n",
    "        ax: plt.Axes = cm.ax_\n",
    "        ax.set_title(f'{self.stage.title()} Epoch: {step}')\n",
    "        return fig\n",
    "\n",
    "    def add_hparams(self, hparams: dict[str, Union[str, Real]], metrics: dict[str, Real]):\n",
    "        _metrics = self._validate_hparam_metric_keys(metrics)\n",
    "        self._writer.add_hparams(hparams, _metrics)\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_hparam_metric_keys(metrics):\n",
    "        _metrics = metrics.copy()\n",
    "        prefix = 'hparam/'\n",
    "        for name in _metrics.keys():\n",
    "            if not name.startswith(prefix):\n",
    "                _metrics[f'{prefix}{name}'] = _metrics[name]\n",
    "                del _metrics[name]\n",
    "        return _metrics\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are more issues around the Experiment Tracker. Let's look at our main.py file again. \n",
    "Here we see that we create an instance of Experiment as an TensorBoardExperiment object. The problem is that we can see that the main.py file calls both abstract *and* non-abstract methods from this object.  This means that we don't have a clean seperation of responsibility between the abstract ExperimentTracker class and the concrete TensorBoardExperiment class. In other words, there's absolutely no real benefit in making the ExperimentTracker an ABC.  Why does this matter? Because at this point, main.py cannot run with any generic type of ExperimentTracker because it is dependent on details from the concrete class.  This is an example  of a *dependency inversion* where a high level class is directly dependent on lower level class details.  This leads to fragile code, which is something we're trying to avoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import get_train_dataloader, get_test_dataloader\n",
    "from src.metrics import Metric\n",
    "from src.models import LinearNet\n",
    "from src.tracking import TensorboardExperiment, Stage\n",
    "from src.utils import generate_tensorboard_experiment_directory\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    'EPOCHS': 20,\n",
    "    'LR': 5e-5,\n",
    "    'OPTIMIZER': 'Adam',\n",
    "    'BATCH_SIZE': 128\n",
    "}\n",
    "\n",
    "# Data\n",
    "train_loader = get_train_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "test_loader = get_test_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "\n",
    "# Model and Optimizer\n",
    "model = LinearNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams.get('LR'))\n",
    "\n",
    "# Objective (loss) function\n",
    "compute_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Metric Containers\n",
    "train_accuracy = Metric()\n",
    "test_accuracy = Metric()\n",
    "y_true_batches = []\n",
    "y_pred_batches = []\n",
    "\n",
    "# Experiment Trackers\n",
    "log_dir = generate_tensorboard_experiment_directory(root='./runs')\n",
    "experiment = TensorboardExperiment(log_dir=log_dir)\n",
    "\n",
    "# Batch Counters\n",
    "test_batch = 0\n",
    "train_batch = 0\n",
    "\n",
    "for epoch in range(hparams.get('EPOCHS')):\n",
    "    # Testing Loop\n",
    "    for x_test, y_test in tqdm(test_loader, desc='Validation Batches', ncols=80):\n",
    "        test_batch += 1\n",
    "        test_batch_size = x_test.shape[0]\n",
    "        test_pred = model(x_test)\n",
    "        loss = compute_loss(test_pred, y_test)\n",
    "\n",
    "        # Compute Batch Validation Metrics\n",
    "        y_test_np = y_test.detach().numpy()\n",
    "        y_test_pred_np = np.argmax(test_pred.detach().numpy(), axis=1)\n",
    "        batch_test_accuracy = accuracy_score(y_test_np, y_test_pred_np)\n",
    "        test_accuracy.update(batch_test_accuracy, test_batch_size)\n",
    "        experiment.set_stage(Stage.VAL)\n",
    "        experiment.add_batch_metric('accuracy', batch_test_accuracy, test_batch)\n",
    "        y_true_batches += [y_test_np]\n",
    "        y_pred_batches += [y_test_pred_np]\n",
    "\n",
    "    # Training Loop\n",
    "    for x_train, y_train in tqdm(train_loader, desc='Train Batches', ncols=80):\n",
    "        train_batch += 1\n",
    "        train_batch_size = x_train.shape[0]\n",
    "        train_pred = model(x_train)\n",
    "        loss = compute_loss(train_pred, y_train)\n",
    "\n",
    "        # Compute Batch Training Metrics\n",
    "        y_train_np = y_train.detach().numpy()\n",
    "        y_train_pred_np = np.argmax(train_pred.detach().numpy(), axis=1)\n",
    "        batch_train_accuracy = accuracy_score(y_train_np, y_train_pred_np)\n",
    "        train_accuracy.update(batch_train_accuracy, train_batch_size)\n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        experiment.add_batch_metric('accuracy', batch_train_accuracy, train_batch)\n",
    "\n",
    "        # Reverse-mode AutoDiff (backpropagation)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute Average Epoch Metrics\n",
    "    summary = ', '.join([\n",
    "        f\"[Epoch: {epoch + 1}/{hparams.get('EPOCHS')}]\",\n",
    "        f\"Test Accuracy: {test_accuracy.average: 0.4f}\",\n",
    "        f\"Train Accuracy: {train_accuracy.average: 0.4f}\",\n",
    "    ])\n",
    "    print('\\n' + summary + '\\n')\n",
    "\n",
    "    # Log Validation Epoch Metrics\n",
    "    experiment.set_stage(Stage.VAL)\n",
    "    experiment.add_epoch_metric('accuracy', test_accuracy.average, epoch)\n",
    "    experiment.add_epoch_confusion_matrix(y_true_batches, y_pred_batches, epoch)\n",
    "\n",
    "    # Log Validation Epoch Metrics\n",
    "    experiment.set_stage(Stage.TRAIN)\n",
    "    experiment.add_epoch_metric('accuracy', train_accuracy.average, epoch)\n",
    "\n",
    "    # Reset metrics\n",
    "    train_accuracy.reset()\n",
    "    test_accuracy.reset()\n",
    "\n",
    "experiment.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can fix this dependency inversion issue.  The first thing we'll do is ditch the abstract base class concept and replacce it with a protocol class.  We'll also move the concrete implementation of set_stage and flush into the protocol class so that it becomes part of the interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from numbers import Real\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple\n",
    "from enum import Enum,  auto()\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Stage(Enum):\n",
    "    TRAIN = auto()\n",
    "    TEST = auto()\n",
    "    VAL = auto()\n",
    "    \n",
    "class ExperimentTracker(Protocol):\n",
    "    \n",
    "# Note that because this is now a protocol class, we can ditch the abstractmethod decorators on the methods.  This \n",
    "# cleans the code up a bit. \n",
    "    \n",
    "\n",
    "  #   @abstractmethod\n",
    "    def add_batch_metric(self, name: str, value: Real, step: int):\n",
    "        \"\"\"Implements logging a batch-level metric.\"\"\"\n",
    "\n",
    " #   @abstractmethod\n",
    "    def add_epoch_metric(self, name: str, value: Real, step: int):\n",
    "        \"\"\"Implements logging a epoch-level metric.\"\"\"\n",
    "\n",
    " #   @abstractmethod\n",
    "    def add_epoch_confusion_matrix(self, y_true: np.array, y_pred: np.array, step: int):\n",
    "        \"\"\"Implements logging a confusion matrix at epoch-level.\"\"\"\n",
    "\n",
    " #   @abstractmethod\n",
    "    def add_hparams(self, hparams: dict[str, Union[str, Real]], metrics: dict[str, Real]):\n",
    "        \"\"\"Implements logging hyperparameters.\"\"\"\n",
    "        \n",
    "    def set_stage(self, stage: Stage):\n",
    "        \"\"\" sets the stage \"\"\"\n",
    "    \n",
    "    def flush(self):\n",
    "       \"\"\" Flushes the experiment\"\"\"\n",
    "\n",
    "# TensorboardExperiment no longer needs to have an inheritance realtionship witih experiment tracker. \n",
    "#class TensorboardExperiment(ExperimentTracker):\n",
    "class TensorboardExperiment: \n",
    "    \n",
    "# Move this from Abstract to concrete implementation. \n",
    "    stage: Stage\n",
    "\n",
    "    def __init__(self, log_dir: str, create=True):\n",
    "        self._validate_log_dir(log_dir, create=create)\n",
    "        self._writer = SummaryWriter(log_dir=log_dir)\n",
    "        plt.ioff()\n",
    "\n",
    "        #Change the stage type here as well. \n",
    "    def set_stage(self, stage: Stage):\n",
    "        self.stage = stage\n",
    "        return self\n",
    "\n",
    "    def flush(self):\n",
    "        self._writer.flush()\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_log_dir(log_dir, create=True):\n",
    "        log_dir = Path(log_dir).resolve()\n",
    "        if log_dir.exists():\n",
    "            return\n",
    "        elif not log_dir.exists() and create:\n",
    "            log_dir.mkdir(parents=True)\n",
    "        else:\n",
    "            raise NotADirectoryError(f'log_dir {log_dir} does not exist.')\n",
    "      #Now that we've set the stage as an enum, we want to print out the stage name, not its value. \n",
    "      # We do this by chaining the name() method to the stage variable.\n",
    "    def add_batch_metric(self, name: str, value: Real, step: int):\n",
    "       # tag = f'{self.stage}/batch/{name}'\n",
    "        tag = f'{self.stage.name}/batch/{name}'\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "        \n",
    "    def add_epoch_metric(self, name: str, value: Real, step: int):\n",
    "        \n",
    "   \n",
    "      # Again, add the .name attribute. \n",
    "      #  tag = f'{self.stage}/epoch/{name}'\n",
    "        tag = f'{self.stage.name}/epoch/{name}'\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def add_epoch_confusion_matrix(self, y_true: list[np.array], y_pred: list[np.array], step: int):\n",
    "        y_true, y_pred = self.collapse_batches(y_true, y_pred)\n",
    "        fig = self.create_confusion_matrix(y_true, y_pred, step)\n",
    "        # Add the name attribute to the stage variable here as well. \n",
    "        # tag = f'{self.stage}/epoch/confusion_matrix'\n",
    "        tag = f'{self.stage.name}/epoch/confusion_matrix'\n",
    "\n",
    "        self._writer.add_figure(tag, fig, step)\n",
    "\n",
    "    @staticmethod\n",
    "    def collapse_batches(y_true: list[np.array], y_pred: list[np.array]) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return np.concatenate(y_true), np.concatenate(y_pred)\n",
    "\n",
    "    def create_confusion_matrix(self, y_true: np.array, y_pred: np.array, step: int) -> plt.Figure:\n",
    "        cm = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred)).plot(cmap='Blues')\n",
    "        fig: plt.Figure = cm.figure_\n",
    "        ax: plt.Axes = cm.ax_\n",
    "        ax.set_title(f'{self.stage.title()} Epoch: {step}')\n",
    "        return fig\n",
    "\n",
    "    def add_hparams(self, hparams: dict[str, Union[str, Real]], metrics: dict[str, Real]):\n",
    "        _metrics = self._validate_hparam_metric_keys(metrics)\n",
    "        self._writer.add_hparams(hparams, _metrics)\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_hparam_metric_keys(metrics):\n",
    "        _metrics = metrics.copy()\n",
    "        prefix = 'hparam/'\n",
    "        for name in _metrics.keys():\n",
    "            if not name.startswith(prefix):\n",
    "                _metrics[f'{prefix}{name}'] = _metrics[name]\n",
    "                del _metrics[name]\n",
    "        return _metrics\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further refactoring can be done here.  There's no need to have the ExperimentTracker and TensorboardExperiment classes in teh same file.  A better organizational structure would be to move the TensorboardExperiment class into its own .py file. \n",
    "Here's what the new tracking.py file looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "from pathlib import Path\n",
    "from typing import Protocol\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Stage(Enum):\n",
    "    TRAIN = auto()\n",
    "    TEST = auto()\n",
    "    VAL = auto()\n",
    "\n",
    "\n",
    "class ExperimentTracker(Protocol):\n",
    "    def set_stage(self, stage: Stage):\n",
    "        \"\"\"Sets the current stage of the experiment.\"\"\"\n",
    "\n",
    "    def add_batch_metric(self, name: str, value: float, step: int):\n",
    "        \"\"\"Implements logging a batch-level metric.\"\"\"\n",
    "\n",
    "    def add_epoch_metric(self, name: str, value: float, step: int):\n",
    "        \"\"\"Implements logging a epoch-level metric.\"\"\"\n",
    "\n",
    "    def add_epoch_confusion_matrix(\n",
    "        self, y_true: list[np.array], y_pred: list[np.array], step: int\n",
    "    ):\n",
    "        \"\"\"Implements logging a confusion matrix at epoch-level.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the new tensorboard.py file looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from ds.tracking import Stage\n",
    "from ds.utils import create_experiment_log_dir\n",
    "\n",
    "#Don't forget to import Stage from the tracking module as we need it for the TensorboardExperiment implementation. \n",
    "from src.tracking import Stage\n",
    "\n",
    "\n",
    "class TensorboardExperiment:\n",
    "    def __init__(self, log_path: str, create: bool = True):\n",
    "\n",
    "        log_dir = create_experiment_log_dir(root=log_path)\n",
    "        self.stage = Stage.TRAIN\n",
    "        self._validate_log_dir(log_dir, create=create)\n",
    "        self._writer = SummaryWriter(log_dir=log_dir)\n",
    "        plt.ioff()\n",
    "\n",
    "    def set_stage(self, stage: Stage):\n",
    "        self.stage = stage\n",
    "\n",
    "    def flush(self):\n",
    "        self._writer.flush()\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_log_dir(log_dir: str, create: bool = True):\n",
    "        log_path = Path(log_dir).resolve()\n",
    "        if log_path.exists():\n",
    "            return\n",
    "        elif not log_path.exists() and create:\n",
    "            log_path.mkdir(parents=True)\n",
    "        else:\n",
    "            raise NotADirectoryError(f\"log_dir {log_dir} does not exist.\")\n",
    "\n",
    "    def add_batch_metric(self, name: str, value: float, step: int):\n",
    "        tag = f\"{self.stage.name}/batch/{name}\"\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def add_epoch_metric(self, name: str, value: float, step: int):\n",
    "        tag = f\"{self.stage.name}/epoch/{name}\"\n",
    "        self._writer.add_scalar(tag, value, step)\n",
    "\n",
    "    def add_epoch_confusion_matrix(\n",
    "        self, y_true: list[np.array], y_pred: list[np.array], step: int\n",
    "    ):\n",
    "        y_true, y_pred = self.collapse_batches(y_true, y_pred)\n",
    "        fig = self.create_confusion_matrix(y_true, y_pred, step)\n",
    "        tag = f\"{self.stage.name}/epoch/confusion_matrix\"\n",
    "\n",
    "    @staticmethod\n",
    "    def collapse_batches(\n",
    "        y_true: list[np.array], y_pred: list[np.array]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        return np.concatenate(y_true), np.concatenate(y_pred)\n",
    "\n",
    "    def create_confusion_matrix(\n",
    "        self, y_true: list[np.array], y_pred: list[np.array], step: int\n",
    "    ) -> plt.Figure:\n",
    "        cm = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred)).plot(cmap=\"Blues\")\n",
    "        cm.ax_.set_title(f\"{self.stage.name} Epoch: {step}\")\n",
    "        return cm.figure_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that splitting these two files from the original tracking.py file makes both of the new files, easier to read and understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget that the main.py file needs to change some import statements because we moved out the Experiment Tracker and the Tensorboard Experiment classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import get_train_dataloader, get_test_dataloader\n",
    "from src.metrics import Metric\n",
    "from src.models import LinearNet\n",
    "from src.tracking import Stage\n",
    "# We need a new import for the TensorboardExperiment class in our main.py file. \n",
    "from src.tensorboard import TensorboardExperiment\n",
    "from src.utils import generate_tensorboard_experiment_directory\n",
    "\n",
    "# ... Rest of main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit vs. implicit typing\n",
    "\n",
    "One thing you may have noticed is that we use both the instrisic float type for our numeric types as well as the Real type. Often times in the code we see these types being used interchangeably, i.e. implicit casting of types.  This is usually not a good coding practice.  It would be better if we were consistent with our numeric types. The code can get away with this because Python will implicitly cast Real types to floats.  But let's fix this problem so that we use explicit typing for all of our variables. \n",
    "\n",
    "In looking through the code, we see that we have the Real Number type used in a number of files, including metric.py and tracking.py.  Let's change those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need this import as we're not going to use the Real type any longer. \n",
    "# from numbers import Real\n",
    "\n",
    "\n",
    "class Metric:\n",
    "    # Change Real to float.\n",
    "    # values: list[Real]\n",
    "    values: list[float]\n",
    "    running_total: float\n",
    "    num_updates: float\n",
    "    average: float\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Metric(average={self.average:0.4f})\"\n",
    "\n",
    " # Change real param to float type.    \n",
    "    # def update(self, value: Real, batch_size: int):\n",
    "    def update (self, value: float, batch_size: int):\n",
    "        self.values.append(value)\n",
    "        self.running_total += value * batch_size\n",
    "        self.num_updates += batch_size\n",
    "        self.average = self.running_total / self.num_updates\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        #Change Real type to float\n",
    "        # self.values: list[Real] = []\n",
    "        self.values: list[float] = []\n",
    "        self.running_total: float = 0.0\n",
    "        self.num_updates: float = 0.0\n",
    "        self.average: float = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Composition.\n",
    "If we look at the method forward() from model.py, it appears like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LinearNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(in_features=28 * 28, out_features=32)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(in_features=32, out_features=10)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in the forward()  method, we use x to store intermediate results before using it to return a final result from the method.  There are understandable reasons for writing code this way.  Mainly, it's inconvenient to have to create new variable ID's for each intermediate result.  However, the real problem is that the x variable will contain different values at different times during the program execution, which can make it difficult to debug issues.  A better way to write this is to use function composition. \n",
    "\n",
    "Most people will remember the concept of function composition from basic algebra, i.e. the idea of having functions like:\n",
    "y = f(g(x)). \n",
    "\n",
    "We cann use this same concept in programming. \n",
    "\n",
    "We could, in theory write the code like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor):\n",
    "   #     x = self.flatten(x)\n",
    "   #     x = self.linear1(x)\n",
    "   #     x = self.relu(x)\n",
    "   #     x = self.linear2(x)\n",
    "   #     x = self.softmax(x)\n",
    "\n",
    "    x = self.softmax(self.linear2(self.relu(self.linear1(self.flatten(x)))))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while functionally correct, this code is ugly and not very elegant.  Also it is difficult to read and doesn't lend itself to more complex pipelines.  \n",
    "\n",
    "Let's look at a better way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import Callable\n",
    "\n",
    "# Here we're creating a new dadta type, ComposableFunction.  ComposableFunction is defined as a callable function\n",
    "# that takes in a list of floating point argument types, and returns a floating point value \n",
    "ComposableFunction = Callable[[float],float]\n",
    "\n",
    "# The Compose function takes a list of these ComposableFunctions and returns a ComposableFunction.\n",
    "# We use the reduce method from functools to handle the pipeline and take care of the intermediate results. \n",
    "def Compose(*functions: ComposableFunction) -> ComposableFunction\n",
    "    return reduce(lambda f,g: lambda x: g(f(x)),functions)\n",
    "\n",
    "class Model:\n",
    "    # Other stuff here. \n",
    "    \n",
    "    # Note here we use the Compose function we've defined above and passed as arguments the functions\n",
    "    # that we want to use to process the data. \n",
    "    # This is a lot more readable. \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        result = Compose(self.flatten,self.linear1,self.relu,self.linear2,self.softmax)\n",
    "        #x = self.flatten(x)\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.linear2(x)\n",
    "       # x = self.softmax(x)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way to do this.  Pytorch has the concept of *sequentials*.  This feature allows developers to use pipelines similar to the way we see above.  The main difference is that the above way is generic to Python and doesn't require importing third party libraries such as Pytorch or Scikit Learn (Which also allows for pipelines). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class LinearNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.network = torch.nn.Sequential( torch.nn.Flatten(),\n",
    "                                            torch.nn.Linear(in_features=28 * 28, out_features = 32),\n",
    "                                            torch.nn.ReLU(),\n",
    "                                            torch.nnLinear(in_features = 32, out_features = 10),\n",
    "                                            torch.nn.Softmax(dim=1))\n",
    "        #self.flatten = torch.nn.Flatten()\n",
    "        #self.linear1 = torch.nn.Linear(in_features=28 * 28, out_features=32)\n",
    "        #self.relu = torch.nn.ReLU()\n",
    "        #self.linear2 = torch.nn.Linear(in_features=32, out_features=10)\n",
    "        #self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        #x = self.flatten(x)\n",
    "        #x = self.linear1(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.linear2(x)\n",
    "        #x = self.softmax(x)\n",
    "        #return x\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take another look at the metric.py module.  Recall that we refactored this by getting rid of the Real number type and converting all of the variables define as such to use the float type instead.  Let's see what else we can refactor.\n",
    "\n",
    "We can immediately see a problem.  In the main.py file, we define two instances of the Metric class, but in looking at the Metric class definition, we see that the attributes are class variables, not instance variables.  This is almost certainly a bug.  We can fix this by turning the Metric class into a dataclass.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We no longer need this import as we're not going to use the Real type any longer. \n",
    "# from numbers import Real\n",
    "\n",
    "\n",
    "class Metric:\n",
    "    # Change Real to float.\n",
    "    # values: list[Real]\n",
    "    values: list[float]\n",
    "    running_total: float\n",
    "    num_updates: float\n",
    "    average: float\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Metric(average={self.average:0.4f})\"\n",
    "\n",
    " # Change real param to float type.    \n",
    "    # def update(self, value: Real, batch_size: int):\n",
    "    def update (self, value: float, batch_size: int):\n",
    "        self.values.append(value)\n",
    "        self.running_total += value * batch_size\n",
    "        self.num_updates += batch_size\n",
    "        self.average = self.running_total / self.num_updates\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        #Change Real type to float\n",
    "        # self.values: list[Real] = []\n",
    "        self.values: list[float] = []\n",
    "        self.running_total: float = 0.0\n",
    "        self.num_updates: float = 0.0\n",
    "        self.average: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "\n",
    "class Metric:\n",
    "    # Change Real to float.\n",
    "    # values: list[Real]\n",
    "    values = field(default_factory = list)\n",
    "    running_total: float = 0.0\n",
    "    num_updates: float = 0.0\n",
    "    average: float = 0.0\n",
    "\n",
    "        \"\"\" The __init__ and __str__ method are no longer needed as the dataclass boilerplate\n",
    "             code provides these methods for us. \"\"\"\n",
    "#    def __init__(self):\n",
    "#        self.reset()\n",
    "\n",
    "#    def __str__(self):\n",
    "#        return f\"Metric(average={self.average:0.4f})\"\n",
    "\n",
    " # Change real param to float type.    \n",
    "    # def update(self, value: Real, batch_size: int):\n",
    "    def update (self, value: float, batch_size: int):\n",
    "        self.values.append(value)\n",
    "        self.running_total += value * batch_size\n",
    "        self.num_updates += batch_size\n",
    "        self.average = self.running_total / self.num_updates\n",
    "\n",
    "\"\"\" We don't need the reset method any longer.  We'll just create a new instance of \n",
    "    the object. \"\"\"\n",
    "\n",
    "#    def reset(self):\n",
    "        \n",
    "#        #Change Real type to float\n",
    "#        # self.values: list[Real] = []\n",
    "#        self.values: list[float] = []\n",
    "#        self.running_total: float = 0.0\n",
    "#        self.num_updates: float = 0.0\n",
    "#        self.average: float = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, the Metric class looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    # Change Real to float.\n",
    "    # values: list[Real]\n",
    "    values: list[float]\n",
    "    running_total: float\n",
    "    num_updates: float\n",
    "    average: float\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Metric(average={self.average:0.4f})\"\n",
    "\n",
    " # Change real param to float type.    \n",
    "    # def update(self, value: Real, batch_size: int):\n",
    "    def update (self, value: float, batch_size: int):\n",
    "        self.values.append(value)\n",
    "        self.running_total += value * batch_size\n",
    "        self.num_updates += batch_size\n",
    "        self.average = self.running_total / self.num_updates\n",
    "\n",
    "    def reset(self):\n",
    "        \n",
    "        #Change Real type to float\n",
    "        # self.values: list[Real] = []\n",
    "        self.values: list[float] = []\n",
    "        self.running_total: float = 0.0\n",
    "        self.num_updates: float = 0.0\n",
    "        self.average: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "\n",
    "class Metric:\n",
    "   \n",
    "    values = field(default_factory = list)\n",
    "    running_total: float = 0.0\n",
    "    num_updates: float = 0.0\n",
    "    average: float = 0.0\n",
    "\n",
    "    def update (self, value: float, batch_size: int):\n",
    "        self.values.append(value)\n",
    "        self.running_total += value * batch_size\n",
    "        self.num_updates += batch_size\n",
    "        self.average = self.running_total / self.num_updates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The refactored Metric class is now substantially simplified. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to also change the main.py file to take the new Metric class code changes.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.dataset import get_train_dataloader, get_test_dataloader\n",
    "from src.metrics import Metric\n",
    "from src.models import LinearNet\n",
    "from src.tracking import TensorboardExperiment, Stage\n",
    "from src.utils import generate_tensorboard_experiment_directory\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    'EPOCHS': 20,\n",
    "    'LR': 5e-5,\n",
    "    'OPTIMIZER': 'Adam',\n",
    "    'BATCH_SIZE': 128\n",
    "}\n",
    "\n",
    "# Data\n",
    "train_loader = get_train_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "test_loader = get_test_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "\n",
    "# Model and Optimizer\n",
    "model = LinearNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams.get('LR'))\n",
    "\n",
    "# Objective (loss) function\n",
    "compute_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Metric Containers\n",
    "train_accuracy = Metric()\n",
    "test_accuracy = Metric()\n",
    "y_true_batches = []\n",
    "y_pred_batches = []\n",
    "\n",
    "# Experiment Trackers\n",
    "log_dir = generate_tensorboard_experiment_directory(root='./runs')\n",
    "experiment = TensorboardExperiment(log_dir=log_dir)\n",
    "\n",
    "# Batch Counters\n",
    "test_batch = 0\n",
    "train_batch = 0\n",
    "\n",
    "for epoch in range(hparams.get('EPOCHS')):\n",
    "    # Testing Loop\n",
    "    for x_test, y_test in tqdm(test_loader, desc='Validation Batches', ncols=80):\n",
    "        test_batch += 1\n",
    "        test_batch_size = x_test.shape[0]\n",
    "        test_pred = model(x_test)\n",
    "        loss = compute_loss(test_pred, y_test)\n",
    "\n",
    "        # Compute Batch Validation Metrics\n",
    "        y_test_np = y_test.detach().numpy()\n",
    "        y_test_pred_np = np.argmax(test_pred.detach().numpy(), axis=1)\n",
    "        batch_test_accuracy = accuracy_score(y_test_np, y_test_pred_np)\n",
    "        test_accuracy.update(batch_test_accuracy, test_batch_size)\n",
    "        experiment.set_stage(Stage.VAL)\n",
    "        experiment.add_batch_metric('accuracy', batch_test_accuracy, test_batch)\n",
    "        y_true_batches += [y_test_np]\n",
    "        y_pred_batches += [y_test_pred_np]\n",
    "\n",
    "    # Training Loop\n",
    "    for x_train, y_train in tqdm(train_loader, desc='Train Batches', ncols=80):\n",
    "        train_batch += 1\n",
    "        train_batch_size = x_train.shape[0]\n",
    "        train_pred = model(x_train)\n",
    "        loss = compute_loss(train_pred, y_train)\n",
    "\n",
    "        # Compute Batch Training Metrics\n",
    "        y_train_np = y_train.detach().numpy()\n",
    "        y_train_pred_np = np.argmax(train_pred.detach().numpy(), axis=1)\n",
    "        batch_train_accuracy = accuracy_score(y_train_np, y_train_pred_np)\n",
    "        train_accuracy.update(batch_train_accuracy, train_batch_size)\n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        experiment.add_batch_metric('accuracy', batch_train_accuracy, train_batch)\n",
    "\n",
    "        # Reverse-mode AutoDiff (backpropagation)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute Average Epoch Metrics\n",
    "    summary = ', '.join([\n",
    "        f\"[Epoch: {epoch + 1}/{hparams.get('EPOCHS')}]\",\n",
    "        f\"Test Accuracy: {test_accuracy.average: 0.4f}\",\n",
    "        f\"Train Accuracy: {train_accuracy.average: 0.4f}\",\n",
    "    ])\n",
    "    print('\\n' + summary + '\\n')\n",
    "\n",
    "    # Log Validation Epoch Metrics\n",
    "    experiment.set_stage(Stage.VAL)\n",
    "    experiment.add_epoch_metric('accuracy', test_accuracy.average, epoch)\n",
    "    experiment.add_epoch_confusion_matrix(y_true_batches, y_pred_batches, epoch)\n",
    "\n",
    "    # Log Validation Epoch Metrics\n",
    "    experiment.set_stage(Stage.TRAIN)\n",
    "    experiment.add_epoch_metric('accuracy', train_accuracy.average, epoch)\n",
    "\n",
    "    # We don't need to call the reset method here.  We just create new instances of the Metric class.\n",
    "    train_accuracy = Metric()\n",
    "    test_accuracy = Metric()\n",
    "\n",
    "experiment.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the Main.py function.  Here we see that main has multiple responsibilities and that much of the code is very repetitive.  For eample, the training and testing loop code is nearly identical, thus violating a core software design principle of DRY - Don't Repeat Yourself.  Let's do some thorough refactoring of the code.  \n",
    "We'll start by creating a new module called Runner.py.  This module will take care of doing the training *and* test looping in one area of responsibility.   \n",
    "\n",
    "Let's create a new module Runner.py.  We can see that in the main.py file, there are variables, test_batch and train_batch which count the number of training and testing runs.  Let's move that into the Runner class.  \n",
    "\n",
    "Additionally, we see that we have a test loader and a training loader.  These are pytorch data loaders.  Let's pass that into the Runner class as a parameter. \n",
    "\n",
    "We  need to pass in the model that the runner module is going to train and test against. \n",
    "\n",
    "The training loop uses the optimizer function, while the test loop doesn't.  So, we'll  need to add an optimizer parameter with a default option in order to make the runner class generic. \n",
    "\n",
    "We will also move the loss function definition out of main.py and into the Runner class. \n",
    "\n",
    "Another modification we'll make is that we'll take the y_true_batches and y_pred_batches variables and move those into the\n",
    "runner class. Well also assign them a type.  They're both arrays of arrays. \n",
    "\n",
    "We need to keep track of the stage that the experiment is in, either TRAIN or VAL. There are multiple ways to achieve this, but we're simply going to assume that if we have optimizer set, we're in the training stages, otherwise we're in the test stages. \n",
    "\n",
    "We'll also put the instantion of the Metric class into the Runner.  That way we can create both a Test Runner and a Train Runner as needed in our main.py file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've created our initializer, we'll want to be able to run the experiment.  Note that we have two parts to this, the training and the testing stage.  \n",
    "\n",
    "First we'll create a run method. Note that in the main.py file there is a description passed in, so we'll need to account for that in the run method.  Also, we'll remove any reference to training or testig here as well.  so, x_train gets renamed to x and y_train gets renamed to y in the run method for loop. \n",
    "\n",
    "Since we need to run this twice, we'll create another method that we'll call _run_single.  This will execute a single run, either a training run or a testing run. Note the leading underscore which makes the method call protected within the class. \n",
    "\n",
    "List of things to do:\n",
    "\n",
    "1.  Rename any variable that has \"train\" or \"test\" as part of the name, since the run_single method will run both testing and training batches. \n",
    "2.  Take the experiment tracker code and move it from the run_single into the main runner control loop. \n",
    "    Note that we need to return the batch_accuracy from the run_single and give that to the experiment tracker \n",
    "    This means that we now need to add a return statement to run_single returning the batch_accuracy method \n",
    "3.  In the main.py file, we compute the average accuracy for the runs, but it is better to put this into a python class property instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-3669730e3719>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-3669730e3719>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    def __init__(self, loader: DataLoader[Any], model: torch.nn.Module, Optional[optimizer: torch.optim.optimizer] = None):\u001b[0m\n\u001b[1;37m                                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from typing import Any, Optional\n",
    "import torch\n",
    "import tqdm\n",
    "from src.tracking import Stage\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self, loader: DataLoader[Any], model: torch.nn.Module, Optional[optimizer: torch.optim.optimizer] = None):\n",
    "        self.run_count = 0  # Replace the train and test batch variables with this one. \n",
    "        self.loader = loader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy_metric = Metric()\n",
    "        self.compute_loss = torch.nn.CrossEntropyLoss(reduction = 'mean')\n",
    "        \n",
    "        # These variables contain the results of the experiment. \n",
    "        self.y_true_batches : list[list[Any]] = [] \n",
    "        self.y_pred_batches : list[list[Any]]= []\n",
    "            \n",
    "        self.stage = Stage.VAL if self.optimizer is None else self.stage = stage.TRAIN\n",
    "        \n",
    "    @property\n",
    "    def avg_accuracy(self) -> float:\n",
    "        return self.accuracy_metric.average\n",
    "        \n",
    "    # Note that in order to make the runner class generic, we need to pass in the ExperimentTracker\n",
    "    # object. \n",
    "    def run(self,desc: str, experiment: ExperimentTracker):\n",
    "        for x,y in tqdm(self.loader, desc = desc, ncols=80):\n",
    "            batch_accuracy = self._run_single(x,y)\n",
    "            experiment.set_stage(Stage.TRAIN)\n",
    "            # Rename batch_train_accuracy and train_batch. \n",
    "            # experiment.add_batch_metric('accuracy', batch_train_accuracy, train_batch)\n",
    "            experiment.add_batch_metric('accuracy', batch_accuracy, self.run_count)\n",
    "            \n",
    "        \n",
    "    def _run_single(self,x: Any,y: Any):\n",
    "        for epoch in range(hparams.get('EPOCHS')):\n",
    "    # Testing Loop\n",
    "    # Let's change x_test and  y_test to x and y\n",
    "    #        for x_test, y_test in tqdm(test_loader, desc='Validation Batches', ncols=80):\n",
    "            for x, y in tqdm(self.loader, desc='Validation Batches', ncols=80):\n",
    "\n",
    "               # test_batch += 1\n",
    "                self.run_count += 1  #We're renaming test_batch to run_count so that the name is more generic. \n",
    "             # change test_batch_suze to just batch_size  and x_test.shape to just x.shape. \n",
    "             # test_batch_size = x_test.shape[0]\n",
    "                batch_size = x.shape[0]\n",
    "                \n",
    "              # Change x_test to just x and test_pred to just prediction. \n",
    "              # test_pred = model(x_test)\n",
    "                prediction = self.model(x)\n",
    "              # Change test_pred to prediciton and y_test to y  \n",
    "              # loss = compute_loss(test_pred, y_test)\n",
    "                loss = compute_loss(prediction, y)\n",
    "\n",
    "        # Compute Batch Validation Metrics\n",
    "        \n",
    "                # Change y_test_np to y_np and y_test_detach() to y_detach\n",
    "                # y_test_np = y_test.detach().numpy()\n",
    "                y_np = y_detach().numpy()\n",
    "                # Change y_test_pred_np to y_pred and test_pred to prediction\n",
    "                # y_test_pred_np = np.argmax(testpred.detach().numpy(), axis=1)\n",
    "                y_np = np.argmax(prediction.detach().numpy(), axis=1)\n",
    "                # Change batch_test_accuracy to batch_accuracy and y_test_npp and y_test_pred_np to y_np and y_pred_np\n",
    "                # batch_test_accuracy = accuracy_score(y_test_np, y_test_pred_np)\n",
    "                batch_accuracy = accuracy_score(y_np, y_pred_np)\n",
    "                test_accuracy.update(batch_test_accuracy, test_batch_size)\n",
    "                experiment.set_stage(Stage.VAL)\n",
    "                experiment.add_batch_metric('accuracy', batch_test_accuracy, test_batch)\n",
    "                # In the following two lines, change y_test_np and y_test_pred_np to y_np and y_pred_np\n",
    "                # Change all variables that have test i the namme.\n",
    "                # y_true_batches += [y_test_np]\n",
    "                # y_pred_batches += [y_test_pred_np]\n",
    "                y_true_batches += [y_np]\n",
    "                y_prediction_batches += [y_prediction_np]\n",
    "                \n",
    "               # This code is now redundant since we don't have separate train and test loops. \n",
    "           # for x_train, y_train in tqdm(train_loader, desc='Train Batches', ncols=80):\n",
    "           #     train_batch += 1\n",
    "           #     train_batch_size = x_train.shape[0]\n",
    "           #     train_pred = model(x_train)\n",
    "           #     loss = compute_loss(train_pred, y_train)\n",
    "       \n",
    "            # Compute Batch Training Metrics\n",
    "                # Change all variables with train in the name. \n",
    "                \n",
    "                # y_train_np = y_train.detach().numpy()\n",
    "                y_np = y.detach().numpy()\n",
    "\n",
    "                # y_train_pred_np = np.argmax(train_pred.detach().numpy(), axis=1)\n",
    "                y_prediction_np = np.argmax(prediction.detach().numpy(), axis=1)\n",
    "\n",
    "                \n",
    "                # batch_train_accuracy = accuracy_score(y_train_np, y_train_pred_np).\n",
    "                # Note we'll make batch_accuracy a float rather than using the default\n",
    "                # return value 'Any'.  Explicit typing is always better than implicit typing.\n",
    "                batch_accuracy: float = accuracy_score(y_np, y_prediction_np)\n",
    "\n",
    "                # Change train_accuracy to self.accuracy. Change batch_train_accuracy and train_batch size \n",
    "                # to train_accuracy and batch_size \n",
    "                # train_accuracy.update(batch_train_accuracy, train_batch_size)\n",
    "                self.accuracy_metric.update(batch_accuracy, batch_size)\n",
    "\n",
    "                \n",
    "\n",
    "            # Reverse-mode AutoDiff (backpropagation)\n",
    "            # Since there may or may not be an optimizer, depending on whether we're training or testing, \n",
    "            # We'll need to add a check here. \n",
    "                if self.optimizer: \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            return batch_accuracy\n",
    "        \n",
    "        # Add a reset method here that can be called from the runner objects in main.py\n",
    "        def reset(self):\n",
    "            self.accuracy = Metric()\n",
    "            y_true_batches = []\n",
    "            y_pred_batches = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our runner class minus all the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from typing import Any, Optional\n",
    "import torch\n",
    "import tqdm\n",
    "from src.tracking import Stage\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self, loader: DataLoader[Any], model: torch.nn.Module, Optional[optimizer: torch.optim.optimizer] = None):\n",
    "        self.run_count = 0  # Replace the train and test batch variables with this one. \n",
    "        self.loader = loader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy_metric = Metric()\n",
    "        self.compute_loss = torch.nn.CrossEntropyLoss(reduction = 'mean')           \n",
    "        self.y_true_batches : list[list[Any]] = [] \n",
    "        self.y_pred_batches : list[list[Any]]= []\n",
    "            \n",
    "        self.stage = Stage.VAL if self.optimizer is None else self.stage = stage.TRAIN\n",
    "    \n",
    "    @property\n",
    "    def avg_accuracy(self) -> float:\n",
    "        return self.accuracy_metric.average\n",
    " \n",
    "    def run(self,desc: str, experiment: ExperimentTracker):\n",
    "        for x,y in tqdm(self.loader, desc = desc, ncols=80):\n",
    "            batch_accuracy = self._run_single(x,y)\n",
    "            experiment.set_stage(Stage.TRAIN)\n",
    "            experiment.add_batch_metric('accuracy', batch_accuracy, self.run_count)\n",
    "                \n",
    "    def _run_single(self,x: Any,y: Any):\n",
    "        for x, y in tqdm(self.loader, desc='Validation Batches', ncols=80):         \n",
    "            self.run_count += 1  #We're renaming test_batch to run_count so that the name is more generic. \n",
    "            batch_size = x.shape[0]\n",
    "            prediction = self.model(x)\n",
    "            loss = compute_loss(prediction, y)\n",
    "                \n",
    "        # Compute Batch Validation Metrics\n",
    "        \n",
    "            y_np = y_detach().numpy()              \n",
    "            y_np = np.argmax(prediction.detach().numpy(), axis=1)                \n",
    "            batch_accuracy = accuracy_score(y_np, y_pred_np)\n",
    "            test_accuracy.update(batch_test_accuracy, test_batch_size)\n",
    "            experiment.set_stage(Stage.VAL)\n",
    "            experiment.add_batch_metric('accuracy', batch_test_accuracy, test_batch)\n",
    "            y_true_batches += [y_np]\n",
    "            y_prediction_batches += [y_prediction_np]                \n",
    "            y_np = y.detach().numpy()             \n",
    "            y_prediction_np = np.argmax(prediction.detach().numpy(), axis=1)          \n",
    "            batch_accuracy: float = accuracy_score(y_np, y_prediction_np)\n",
    "            self.accuracy_metric.update(batch_accuracy, batch_size)            \n",
    "\n",
    "            # Reverse-mode AutoDiff (backpropagation)\n",
    "          \n",
    "            if self.optimizer: \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "        return batch_accuracy\n",
    "    \n",
    "     def reset(self):\n",
    "            self.accuracy = Metric()\n",
    "            y_true_batches = []\n",
    "            y_pred_batches = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our runner class, let's go back to our main.py and refactor it to use this module rather than running everything from main explicitly.  \n",
    "The first thing that we'll do is actualy create a main() function.  While Python doesn't require this, unlike languages such as  C, C++ or Java, it's actually a good thing to do from an organizational point.\n",
    "Let's look at our main.py file again.\n",
    "We'll also need a condition at the bottom which ensure that the main method will actually be run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# We don't need the following two imports in main.py any longer. \n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from src.dataset import get_train_dataloader, get_test_dataloader\n",
    "from src.metrics import Metric\n",
    "from src.models import LinearNet\n",
    "from src.tracking import TensorboardExperiment, Stage\n",
    "from src.utils import generate_tensorboard_experiment_directory\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    'EPOCHS': 20,\n",
    "    'LR': 5e-5,\n",
    "    'OPTIMIZER': 'Adam',\n",
    "    'BATCH_SIZE': 128\n",
    "}\n",
    "\n",
    "# Let's create a main method here. \n",
    "\n",
    "def main():\n",
    "    # Data\n",
    "    train_loader = get_train_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "    test_loader = get_test_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "\n",
    "    # Model and Optimizer\n",
    "    model = LinearNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hparams.get('LR'))\n",
    "\n",
    "    \n",
    "    # Note that the compute_loss funtion has already been moved into the runner objecr, so we no longer\n",
    "    # need it in our main function. \n",
    "    # Objective (loss) function\n",
    "    # compute_loss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "    \n",
    "    # Here we can now create our runner objects. \n",
    "    test_runner = Runner(test_loader,model)\n",
    "    train_runner = Runner(train_loader, model, optimizer) # Remember that the train runner needs the optimizer. \n",
    "\n",
    "    \"\"\" We don't need any of the metric container code.  It's all been moved into the runner.\n",
    "    # Metric Containers\n",
    "    train_accuracy = Metric()\n",
    "    test_accuracy = Metric()\n",
    "    y_true_batches = []\n",
    "    y_pred_batches = []\n",
    "    \"\"\"\n",
    "\n",
    "    # Experiment Trackers\n",
    "    log_dir = generate_tensorboard_experiment_directory(root='./runs')\n",
    "    experiment = TensorboardExperiment(log_dir=log_dir)\n",
    "\n",
    "    \"\"\" We don't need this code.  It's been moved into the Runner class. \n",
    "    # Batch Counters\n",
    "    test_batch = 0\n",
    "    train_batch = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" All of this code has been moved into the runner class. \n",
    "    for epoch in range(hparams.get('EPOCHS')):\n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        train_runner.run('Train batches',experiment)\n",
    "        experiment.set_stage(Stage.VAL)\n",
    "        test_runner.run('Test batches',experiment)\n",
    "        \n",
    "    # Testing Loop\n",
    "        for x_test, y_test in tqdm(test_loader, desc='Validation Batches', ncols=80):\n",
    "            test_batch += 1\n",
    "            test_batch_size = x_test.shape[0]\n",
    "            test_pred = model(x_test)\n",
    "            loss = compute_loss(test_pred, y_test)\n",
    "\n",
    "        # Compute Batch Validation Metrics\n",
    "            y_test_np = y_test.detach().numpy()\n",
    "            y_test_pred_np = np.argmax(test_pred.detach().numpy(), axis=1)\n",
    "            batch_test_accuracy = accuracy_score(y_test_np, y_test_pred_np)\n",
    "            test_accuracy.update(batch_test_accuracy, test_batch_size)\n",
    "            experiment.set_stage(Stage.VAL)\n",
    "            experiment.add_batch_metric('accuracy', batch_test_accuracy, test_batch)\n",
    "            y_true_batches += [y_test_np]\n",
    "            y_pred_batches += [y_test_pred_np]\n",
    "\n",
    "        # Training Loop\n",
    "        for x_train, y_train in tqdm(train_loader, desc='Train Batches', ncols=80):\n",
    "            train_batch += 1\n",
    "            train_batch_size = x_train.shape[0]\n",
    "            train_pred = model(x_train)\n",
    "            loss = compute_loss(train_pred, y_train)\n",
    "\n",
    "            # Compute Batch Training Metrics\n",
    "            y_train_np = y_train.detach().numpy()\n",
    "            y_train_pred_np = np.argmax(train_pred.detach().numpy(), axis=1)\n",
    "            batch_train_accuracy = accuracy_score(y_train_np, y_train_pred_np)\n",
    "            train_accuracy.update(batch_train_accuracy, train_batch_size)\n",
    "            experiment.set_stage(Stage.TRAIN)\n",
    "            experiment.add_batch_metric('accuracy', batch_train_accuracy, train_batch)\n",
    "\n",
    "            # Reverse-mode AutoDiff (backpropagation)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\"\"\"\n",
    "        # Compute Average Epoch Metrics\n",
    "        # Note that we've created a property in the runner class, so let's use that to compute the averrage. \n",
    "        \"\"\"\n",
    "        summary = ', '.join([\n",
    "            f\"[Epoch: {epoch + 1}/{hparams.get('EPOCHS')}]\",\n",
    "            f\"Test Accuracy: {test_accuracy.average: 0.4f}\",\n",
    "            f\"Train Accuracy: {train_accuracy.average: 0.4f}\",\n",
    "        ])\n",
    "        \"\"\"\n",
    "    for epoch in range(hparams.get('EPOCHS')):\n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        train_runner.run('Train batches', experiment)\n",
    "        experiment.set_stage(Stage.VAL)\n",
    "        test_runner,run('Test batches', experiment)\n",
    "        summary = ', '.join([\n",
    "            f\"[Epoch: {epoch + 1}/{hparams.get('EPOCHS')}]\",\n",
    "            f\"Test Accuracy: {test_runner.avg_accuracy: 0.4f}\",\n",
    "            f\"Train Accuracy: {train_runner.avg_accuracy: 0.4f}\",\n",
    "        ])\n",
    "        print('\\n' + summary + '\\n')\n",
    "\n",
    "        # Log Validation Epoch Metrics\n",
    "        # We don't need this any more since we use generic runners instead. \n",
    "        # experiment.set_stage(Stage.VAL)\n",
    "        \n",
    "        experiment.add_epoch_metric('accuracy', test_runner.avg_accuracy, epoch)\n",
    "        experiment.add_epoch_metric('accuracy', train_runner.avg_accuracy, epoch)\n",
    "        # y_true_batches and y_pred_batches are now in the runner class.\n",
    "        # experiment.add_epoch_confusion_matrix(y_true_batches, y_pred_batches, epoch)\n",
    "        experiment.add_epoch_confusion_matrix(test_runner.y_true_batches, test_runner.y_pred_batches, epoch)\n",
    "\n",
    "\n",
    "        # Log Validation Epoch Metrics\n",
    "        \"\"\" We've moved this up.  This code is no longer needed here. \n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        experiment.add_epoch_metric('accuracy', train_accuracy.average, epoch)\n",
    "        \"\"\"\n",
    "\n",
    "        train_runner.reset()\n",
    "        test_runner.reset()\n",
    "\n",
    "    experiment.flush()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the new runner class minus the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.dataset import get_train_dataloader, get_test_dataloader\n",
    "from src.metrics import Metric\n",
    "from src.models import LinearNet\n",
    "from src.tracking import TensorboardExperiment, Stage\n",
    "from src.utils import generate_tensorboard_experiment_directory\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    'EPOCHS': 20,\n",
    "    'LR': 5e-5,\n",
    "    'OPTIMIZER': 'Adam',\n",
    "    'BATCH_SIZE': 128\n",
    "}\n",
    "\n",
    "# Let's create a main method here. \n",
    "\n",
    "def main():\n",
    "    # Data\n",
    "    train_loader = get_train_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "    test_loader = get_test_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "\n",
    "    # Model and Optimizer\n",
    "    model = LinearNet()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hparams.get('LR'))\n",
    "\n",
    "    # Here we can now create our runner objects. \n",
    "    test_runner = Runner(test_loader,model)\n",
    "    train_runner = Runner(train_loader, model, optimizer) # Remember that the train runner needs the optimizer. \n",
    "\n",
    "    # Experiment Trackers\n",
    "    log_dir = generate_tensorboard_experiment_directory(root='./runs')\n",
    "    experiment = TensorboardExperiment(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(hparams.get('EPOCHS')):\n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        train_runner.run('Train batches', experiment)\n",
    "        experiment.set_stage(Stage.VAL)\n",
    "        test_runner,run('Test batches', experiment)\n",
    "       \n",
    "        # Compute Average Epoch Metrics       \n",
    "        \n",
    "        summary = ', '.join([\n",
    "            f\"[Epoch: {epoch + 1}/{hparams.get('EPOCHS')}]\",\n",
    "            f\"Test Accuracy: {test_runner.avg_accuracy: 0.4f}\",\n",
    "            f\"Train Accuracy: {train_runner.avg_accuracy: 0.4f}\",\n",
    "        ])\n",
    "        print('\\n' + summary + '\\n')\n",
    "\n",
    "        experiment.add_epoch_metric('accuracy', test_runner.avg_accuracy, epoch)\n",
    "        experiment.add_epoch_metric('accuracy', train_runner.avg_accuracy, epoch)       \n",
    "        experiment.add_epoch_confusion_matrix(test_runner.y_true_batches, test_runner.y_pred_batches, epoch)\n",
    "\n",
    "        train_runner.reset()\n",
    "        test_runner.reset()\n",
    "\n",
    "    experiment.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've done quite a lot of refactoring of the main.py file, moving much of the code out into a separate runner class. \n",
    "Let's go a bit further with this.  We still have a lot of code in main.py that is concerened with running the experiment  We should move all of that into the runner class via the principle of :*single responsibility*.  Not that the run_epoch method takes an epoch_id, so we need to change epoch to epoch_id in the code. \n",
    "\n",
    "We'll also want one more parameter to run_epoch.  This will be the epoch_total which will be an integer. \n",
    "\n",
    "We'll create a new method in the runner class, *run_epoch()*.  We'll move the remaining experiment running code into this run_epoch method.  Here's an updated runner class. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from typing import Any, Optional\n",
    "import torch\n",
    "import tqdm\n",
    "from src.tracking import Stage\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self, loader: DataLoader[Any], model: torch.nn.Module, Optional[optimizer: torch.optim.optimizer] = None):\n",
    "        self.run_count = 0  # Replace the train and test batch variables with this one. \n",
    "        self.loader = loader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy_metric = Metric()\n",
    "        self.compute_loss = torch.nn.CrossEntropyLoss(reduction = 'mean')           \n",
    "        self.y_true_batches : list[list[Any]] = [] \n",
    "        self.y_pred_batches : list[list[Any]]= []\n",
    "            \n",
    "        self.stage = Stage.VAL if self.optimizer is None else self.stage = stage.TRAIN\n",
    "    \n",
    "    @property\n",
    "    def avg_accuracy(self) -> float:\n",
    "        return self.accuracy_metric.average\n",
    " \n",
    "    def run(self,desc: str, experiment: ExperimentTracker):\n",
    "        for x,y in tqdm(self.loader, desc = desc, ncols=80):\n",
    "            batch_accuracy = self._run_single(x,y)\n",
    "            experiment.set_stage(Stage.TRAIN)\n",
    "            experiment.add_batch_metric('accuracy', batch_accuracy, self.run_count)\n",
    "                \n",
    "    def _run_single(self,x: Any,y: Any):\n",
    "        for x, y in tqdm(self.loader, desc='Validation Batches', ncols=80):         \n",
    "            self.run_count += 1  #We're renaming test_batch to run_count so that the name is more generic. \n",
    "            batch_size = x.shape[0]\n",
    "            prediction = self.model(x)\n",
    "            loss = compute_loss(prediction, y)\n",
    "                \n",
    "        # Compute Batch Validation Metrics\n",
    "        \n",
    "            y_np = y_detach().numpy()              \n",
    "            y_np = np.argmax(prediction.detach().numpy(), axis=1)                \n",
    "            batch_accuracy = accuracy_score(y_np, y_pred_np)\n",
    "            test_accuracy.update(batch_test_accuracy, test_batch_size)\n",
    "            experiment.set_stage(Stage.VAL)\n",
    "            experiment.add_batch_metric('accuracy', batch_test_accuracy, test_batch)\n",
    "            y_true_batches += [y_np]\n",
    "            y_prediction_batches += [y_prediction_np]                \n",
    "            y_np = y.detach().numpy()             \n",
    "            y_prediction_np = np.argmax(prediction.detach().numpy(), axis=1)          \n",
    "            batch_accuracy: float = accuracy_score(y_np, y_prediction_np)\n",
    "            self.accuracy_metric.update(batch_accuracy, batch_size)            \n",
    "\n",
    "            # Reverse-mode AutoDiff (backpropagation)\n",
    "          \n",
    "            if self.optimizer: \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "        return batch_accuracy\n",
    "    \n",
    "    # This method takes most of the main.py code and moves it into the Runner class. \n",
    "    def run_epoch(self, test_runner: Runner, train_runner: Runner, experiment: ExperimentTracker epoch_id: int,\n",
    "                 epoch_total: int):\n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        train_runner.run('Train batches', experiment)\n",
    "        experiment.set_stage(Stage.VAL)\n",
    "        test_runner,run('Test batches', experiment)\n",
    "       \n",
    "        # Compute Average Epoch Metrics       \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Change epoch to epoch_id everywhere. Allso, since we're using epoch_total, we don't need to retrieve the EPOCHS \n",
    "        # from the hparams dictionary. \n",
    "        summary = ', '.join([\n",
    "           # f\"[Epoch: {epoch + 1}/{hparams.get('EPOCHS')}]\",\n",
    "            f\"[Epoch: {epoch_id + 1}/{epoch_total}\",\n",
    "\n",
    "            f\"Test Accuracy: {test_runner.avg_accuracy: 0.4f}\",\n",
    "            f\"Train Accuracy: {train_runner.avg_accuracy: 0.4f}\",\n",
    "        ])\n",
    "        print('\\n' + summary + '\\n')\n",
    "\n",
    "        # experiment.add_epoch_metric('accuracy', test_runner.avg_accuracy, epoch)\n",
    "        # experiment.add_epoch_metric('accuracy', train_runner.avg_accuracy, epoch)       \n",
    "        # experiment.add_epoch_confusion_matrix(test_runner.y_true_batches, test_runner.y_pred_batches, epoch)\n",
    "        \n",
    "        experiment.add_epoch_metric('accuracy', test_runner.avg_accuracy, epoch_id)\n",
    "        experiment.add_epoch_metric('accuracy', train_runner.avg_accuracy, epoch_id)       \n",
    "        experiment.add_epoch_confusion_matrix(test_runner.y_true_batches, test_runner.y_pred_batches, epoch_id)\n",
    "\n",
    "        train_runner.reset()\n",
    "        test_runner.reset()\n",
    " \n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.accuracy = Metric()\n",
    "        y_true_batches = []\n",
    "        y_pred_batches = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,  in main, we can get rid of most of our code.  Additionally, the storing of hyperparameters in a dictionary is a bit awkward and non-standard.  Ir would be better to simply make them constants.  We'll also add a logpath constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.dataset import get_train_dataloader, get_test_dataloader\n",
    "from src.metrics import Metric\n",
    "from src.models import LinearNet\n",
    "from src.tracking import TensorboardExperiment, Stage\n",
    "from src.utils import generate_tensorboard_experiment_directory\n",
    "\n",
    "# Hyperparameters\n",
    "# Let's get rid of the dictionary and make these constant values instead. \n",
    "\"\"\"\n",
    "hparams = {\n",
    "    'EPOCHS': 20,\n",
    "    'LR': 5e-5,\n",
    "    'OPTIMIZER': 'Adam',\n",
    "    'BATCH_SIZE': 128\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "EPOCH_COuNT = 20\n",
    "LR = 5e-5\n",
    "OPTIMIZER = 'ADAM'\n",
    "BATCH_SIZE = 128\n",
    "LOG_PATH = './runs'\n",
    "\n",
    "# Let's create a main method here. \n",
    "\n",
    "def main():\n",
    "    # Data\n",
    "    #Swap out dictionary lookups for constants. \n",
    "    # train_loader = get_train_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "    # test_loader = get_test_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "    \n",
    "    train_loader = get_train_dataloader(batch_size = BATCH_SIZE)\n",
    "    test_loader = get_test_dataloader(batch_size = BATCH_SIZE\n",
    "\n",
    "    # Model and Optimizer\n",
    "    model = LinearNet()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=hparams.get('LR'))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR\n",
    "\n",
    "    # Here we can now create our runner objects. \n",
    "    test_runner = Runner(test_loader,model)\n",
    "    train_runner = Runner(train_loader, model, optimizer) # Remember that the train runner needs the optimizer. \n",
    "\n",
    "    # Experiment Trackers\n",
    "    # Note that the log_dir definition uses a string constant to specify the log directory. \n",
    "    # Let's use the LOG_PATH constant instead. \n",
    "    # log_dir = generate_tensorboard_experiment_directory(root='./runs')\n",
    "    log_dir = generate_tensorboard_experiment_directory(root=LOG_PATH)\n",
    "\n",
    "                                 \n",
    "    experiment = TensorboardExperiment(log_dir=log_dir)\n",
    "\n",
    "    # for epoch_id in range(hparams.get('EPOCHS')):\n",
    "    for epoch_id in range(EPOCH_COUNT):\n",
    "        run_epoch(test_runner, train_runner, experiment, epoch_id, EPOCH_COUNT)\n",
    "        \n",
    "       \n",
    "    experiment.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from typing import Any, Optional\n",
    "import torch\n",
    "import tqdm\n",
    "from src.tracking import Stage\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self, loader: DataLoader[Any], model: torch.nn.Module, Optional[optimizer: torch.optim.optimizer] = None):\n",
    "        self.run_count = 0  # Replace the train and test batch variables with this one. \n",
    "        self.loader = loader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy_metric = Metric()\n",
    "        self.compute_loss = torch.nn.CrossEntropyLoss(reduction = 'mean')           \n",
    "        self.y_true_batches : list[list[Any]] = [] \n",
    "        self.y_pred_batches : list[list[Any]]= []\n",
    "            \n",
    "        self.stage = Stage.VAL if self.optimizer is None else self.stage = stage.TRAIN\n",
    "    \n",
    "    @property\n",
    "    def avg_accuracy(self) -> float:\n",
    "        return self.accuracy_metric.average\n",
    " \n",
    "    def run(self,desc: str, experiment: ExperimentTracker):\n",
    "        for x,y in tqdm(self.loader, desc = desc, ncols=80):\n",
    "            batch_accuracy = self._run_single(x,y)\n",
    "            experiment.set_stage(Stage.TRAIN)\n",
    "            experiment.add_batch_metric('accuracy', batch_accuracy, self.run_count)\n",
    "                \n",
    "    def _run_single(self,x: Any,y: Any):\n",
    "        for x, y in tqdm(self.loader, desc='Validation Batches', ncols=80):         \n",
    "            self.run_count += 1  #We're renaming test_batch to run_count so that the name is more generic. \n",
    "            batch_size = x.shape[0]\n",
    "            prediction = self.model(x)\n",
    "            loss = compute_loss(prediction, y)\n",
    "                \n",
    "        # Compute Batch Validation Metrics\n",
    "        \n",
    "            y_np = y_detach().numpy()              \n",
    "            y_np = np.argmax(prediction.detach().numpy(), axis=1)                \n",
    "            batch_accuracy = accuracy_score(y_np, y_pred_np)\n",
    "            test_accuracy.update(batch_test_accuracy, test_batch_size)\n",
    "            experiment.set_stage(Stage.VAL)\n",
    "            experiment.add_batch_metric('accuracy', batch_test_accuracy, test_batch)\n",
    "            y_true_batches += [y_np]\n",
    "            y_prediction_batches += [y_prediction_np]                \n",
    "            y_np = y.detach().numpy()             \n",
    "            y_prediction_np = np.argmax(prediction.detach().numpy(), axis=1)          \n",
    "            batch_accuracy: float = accuracy_score(y_np, y_prediction_np)\n",
    "            self.accuracy_metric.update(batch_accuracy, batch_size)            \n",
    "\n",
    "            # Reverse-mode AutoDiff (backpropagation)\n",
    "          \n",
    "            if self.optimizer: \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "        return batch_accuracy\n",
    "    \n",
    "    # This method takes most of the main.py code and moves it into the Runner class. \n",
    "    def run_epoch(self, test_runner: Runner, train_runner: Runner, experiment: ExperimentTracker epoch_id: int,\n",
    "                 epoch_total: int):\n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        train_runner.run('Train batches', experiment)\n",
    "        experiment.set_stage(Stage.VAL)\n",
    "        test_runner,run('Test batches', experiment)\n",
    "       \n",
    "        # Compute Average Epoch Metrics       \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Change epoch to epoch_id everywhere. Allso, since we're using epoch_total, we don't need to retrieve the EPOCHS \n",
    "        # from the hparams dictionary. \n",
    "        summary = ', '.join([\n",
    "           # f\"[Epoch: {epoch + 1}/{hparams.get('EPOCHS')}]\",\n",
    "            f\"[Epoch: {epoch_id + 1}/{epoch_total}\",\n",
    "\n",
    "            f\"Test Accuracy: {test_runner.avg_accuracy: 0.4f}\",\n",
    "            f\"Train Accuracy: {train_runner.avg_accuracy: 0.4f}\",\n",
    "        ])\n",
    "        print('\\n' + summary + '\\n')\n",
    "\n",
    "        # experiment.add_epoch_metric('accuracy', test_runner.avg_accuracy, epoch)\n",
    "        # experiment.add_epoch_metric('accuracy', train_runner.avg_accuracy, epoch)       \n",
    "        # experiment.add_epoch_confusion_matrix(test_runner.y_true_batches, test_runner.y_pred_batches, epoch)\n",
    "        \n",
    "        experiment.add_epoch_metric('accuracy', test_runner.avg_accuracy, epoch_id)\n",
    "        experiment.add_epoch_metric('accuracy', train_runner.avg_accuracy, epoch_id)       \n",
    "        experiment.add_epoch_confusion_matrix(test_runner.y_true_batches, test_runner.y_pred_batches, epoch_id)\n",
    "\n",
    "        train_runner.reset()\n",
    "        test_runner.reset()\n",
    " \n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.accuracy = Metric()\n",
    "        y_true_batches = []\n",
    "        y_pred_batches = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the new runner class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from typing import Any, Optional\n",
    "import torch\n",
    "import tqdm\n",
    "from src.tracking import Stage\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self, loader: DataLoader[Any], model: torch.nn.Module, Optional[optimizer: torch.optim.optimizer] = None):\n",
    "        self.run_count = 0  # Replace the train and test batch variables with this one. \n",
    "        self.loader = loader\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy_metric = Metric()\n",
    "        self.compute_loss = torch.nn.CrossEntropyLoss(reduction = 'mean')           \n",
    "        self.y_true_batches : list[list[Any]] = [] \n",
    "        self.y_pred_batches : list[list[Any]]= []\n",
    "            \n",
    "        self.stage = Stage.VAL if self.optimizer is None else self.stage = stage.TRAIN\n",
    "    \n",
    "    @property\n",
    "    def avg_accuracy(self) -> float:\n",
    "        return self.accuracy_metric.average\n",
    " \n",
    "    def run(self,desc: str, experiment: ExperimentTracker):\n",
    "        for x,y in tqdm(self.loader, desc = desc, ncols=80):\n",
    "            batch_accuracy = self._run_single(x,y)\n",
    "            experiment.set_stage(Stage.TRAIN)\n",
    "            experiment.add_batch_metric('accuracy', batch_accuracy, self.run_count)\n",
    "                \n",
    "    def _run_single(self,x: Any,y: Any):\n",
    "        for x, y in tqdm(self.loader, desc='Validation Batches', ncols=80):         \n",
    "            self.run_count += 1  #We're renaming test_batch to run_count so that the name is more generic. \n",
    "            batch_size = x.shape[0]\n",
    "            prediction = self.model(x)\n",
    "            loss = compute_loss(prediction, y)\n",
    "                \n",
    "        # Compute Batch Validation Metrics\n",
    "        \n",
    "            y_np = y_detach().numpy()              \n",
    "            y_np = np.argmax(prediction.detach().numpy(), axis=1)                \n",
    "            batch_accuracy = accuracy_score(y_np, y_pred_np)\n",
    "            test_accuracy.update(batch_test_accuracy, test_batch_size)\n",
    "            experiment.set_stage(Stage.VAL)\n",
    "            experiment.add_batch_metric('accuracy', batch_test_accuracy, test_batch)\n",
    "            y_true_batches += [y_np]\n",
    "            y_prediction_batches += [y_prediction_np]                \n",
    "            y_np = y.detach().numpy()             \n",
    "            y_prediction_np = np.argmax(prediction.detach().numpy(), axis=1)          \n",
    "            batch_accuracy: float = accuracy_score(y_np, y_prediction_np)\n",
    "            self.accuracy_metric.update(batch_accuracy, batch_size)            \n",
    "\n",
    "            # Reverse-mode AutoDiff (backpropagation)\n",
    "          \n",
    "            if self.optimizer: \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                    \n",
    "        return batch_accuracy\n",
    "    \n",
    "    # This method takes most of the main.py code and moves it into the Runner class. \n",
    "    def run_epoch(self, test_runner: Runner, train_runner: Runner, experiment: ExperimentTracker epoch_id: int,\n",
    "                 epoch_total: int):\n",
    "        experiment.set_stage(Stage.TRAIN)\n",
    "        train_runner.run('Train batches', experiment)\n",
    "        experiment.set_stage(Stage.VAL)\n",
    "        test_runner,run('Test batches', experiment)\n",
    "       \n",
    "        # Compute Average Epoch Metrics       \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Change epoch to epoch_id everywhere. Allso, since we're using epoch_total, we don't need to retrieve the EPOCHS \n",
    "        # from the hparams dictionary. \n",
    "        summary = ', '.join([\n",
    "           # f\"[Epoch: {epoch + 1}/{hparams.get('EPOCHS')}]\",\n",
    "            f\"[Epoch: {epoch_id + 1}/{epoch_total}\",\n",
    "\n",
    "            f\"Test Accuracy: {test_runner.avg_accuracy: 0.4f}\",\n",
    "            f\"Train Accuracy: {train_runner.avg_accuracy: 0.4f}\",\n",
    "        ])\n",
    "        print('\\n' + summary + '\\n')\n",
    "\n",
    "        # experiment.add_epoch_metric('accuracy', test_runner.avg_accuracy, epoch)\n",
    "        # experiment.add_epoch_metric('accuracy', train_runner.avg_accuracy, epoch)       \n",
    "        # experiment.add_epoch_confusion_matrix(test_runner.y_true_batches, test_runner.y_pred_batches, epoch)\n",
    "        \n",
    "        experiment.add_epoch_metric('accuracy', test_runner.avg_accuracy, epoch_id)\n",
    "        experiment.add_epoch_metric('accuracy', train_runner.avg_accuracy, epoch_id)       \n",
    "        experiment.add_epoch_confusion_matrix(test_runner.y_true_batches, test_runner.y_pred_batches, epoch_id)\n",
    "\n",
    "        train_runner.reset()\n",
    "        test_runner.reset()\n",
    "  \n",
    "    def reset(self):\n",
    "        self.accuracy = Metric()\n",
    "        y_true_batches = []\n",
    "        y_pred_batches = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is our new main.py file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.dataset import get_train_dataloader, get_test_dataloader\n",
    "from src.metrics import Metric\n",
    "from src.models import LinearNet\n",
    "from src.tracking import TensorboardExperiment, Stage\n",
    "from src.utils import generate_tensorboard_experiment_directory\n",
    "\n",
    "# Hyperparameters\n",
    "# Let's get rid of the dictionary and make these constant values instead. \n",
    "\"\"\"\n",
    "hparams = {\n",
    "    'EPOCHS': 20,\n",
    "    'LR': 5e-5,\n",
    "    'OPTIMIZER': 'Adam',\n",
    "    'BATCH_SIZE': 128\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "EPOCH_COuNT = 20\n",
    "LR = 5e-5\n",
    "OPTIMIZER = 'ADAM'\n",
    "BATCH_SIZE = 128\n",
    "LOG_PATH = './runs'\n",
    "\n",
    "# Let's create a main method here. \n",
    "\n",
    "def main():\n",
    "    # Data\n",
    "    #Swap out dictionary lookups for constants. \n",
    "    # train_loader = get_train_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "    # test_loader = get_test_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "    \n",
    "    train_loader = get_train_dataloader(batch_size = BATCH_SIZE)\n",
    "    test_loader = get_test_dataloader(batch_size = BATCH_SIZE\n",
    "\n",
    "    # Model and Optimizer\n",
    "    model = LinearNet()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=hparams.get('LR'))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR\n",
    "\n",
    "    # Here we can now create our runner objects. \n",
    "    test_runner = Runner(test_loader,model)\n",
    "    train_runner = Runner(train_loader, model, optimizer) # Remember that the train runner needs the optimizer. \n",
    "\n",
    "    # Experiment Trackers\n",
    "    # Note that the log_dir definition uses a string constant to specify the log directory. \n",
    "    # Let's use the LOG_PATH constant instead. \n",
    "    # log_dir = generate_tensorboard_experiment_directory(root='./runs')\n",
    "    log_dir = generate_tensorboard_experiment_directory(root=LOG_PATH)\n",
    "\n",
    "                                 \n",
    "    experiment = TensorboardExperiment(log_dir=log_dir)\n",
    "\n",
    "    # for epoch_id in range(hparams.get('EPOCHS')):\n",
    "    for epoch_id in range(EPOCH_COUNT):\n",
    "        run_epoch(test_runner, train_runner, experiment, epoch_id, EPOCH_COUNT)\n",
    "        \n",
    "       \n",
    "    experiment.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're getting close to the end here.  We've managed to get our constant configuratio variables set up in main.py, but, are there any other hidden configuratio dependencies ahywhere in our code?  Let's take a look at the loader.py file. \n",
    "\n",
    "We can see that there are hard-coded file paths sprinkled throughout the code.  This now creates a dependency inversion \n",
    "between the low-level module (loader.py) and the higher level code that uses it.  Since depedency inversions are what we're trying to avoid, we need to change the code. \n",
    "\n",
    "Our first task is to move all of the hard coded file paths out of the loader module an into main.oy as part of the configuratrion. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Move this code into main.py configuration section. \n",
    "# DATA_DIR = (Path(__file__).parent / \"../data\").resolve()\n",
    "\n",
    "ALLOWED_TYPES = {\n",
    "    \"UNSIGNED_BYTE\": b\"\\x08\",\n",
    "    \"SIGNED_BYTE\": b\"\\x09\",\n",
    "    \"SHORT\": b\"\\x0B\",\n",
    "    \"INT\": b\"\\x0C\",\n",
    "    \"SINGLE\": b\"\\x0D\",\n",
    "    \"DOUBLE\": b\"\\x0E\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    with gzip.open(DATA_DIR / \"t10k-images-idx3-ubyte.gz\", \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 3\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_images == 10_000\n",
    "\n",
    "        (num_rows,) = struct.unpack(\">I\", fp.read(4))\n",
    "        (num_cols,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_rows == num_cols == 28\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images * num_rows * num_cols\n",
    "\n",
    "    data = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    data = data.reshape((num_images, num_rows, num_cols))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    with gzip.open(DATA_DIR / \"train-images-idx3-ubyte.gz\", \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 3\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_images == 60_000\n",
    "\n",
    "        (num_rows,) = struct.unpack(\">I\", fp.read(4))\n",
    "        (num_cols,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_rows == num_cols == 28\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images * num_rows * num_cols\n",
    "\n",
    "    data = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    data = data.reshape((num_images, num_rows, num_cols))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_test_labels():\n",
    "    with gzip.open(DATA_DIR / \"t10k-labels-idx1-ubyte.gz\", \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 1\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_images == 10_000\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images\n",
    "\n",
    "    data = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_train_labels():\n",
    "    with gzip.open(DATA_DIR / \"train-labels-idx1-ubyte.gz\", \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 1\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "        assert num_images == 60_000\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images\n",
    "\n",
    "    data = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our main.py file we're going to store the data directory and then have constants for each type of testing and training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pathlib  # Need this to construct the path for the data, \n",
    "\n",
    "from src.dataset import get_train_dataloader, get_test_dataloader\n",
    "from src.metrics import Metric\n",
    "from src.models import LinearNet\n",
    "from src.tracking import TensorboardExperiment, Stage\n",
    "from src.utils import generate_tensorboard_experiment_directory\n",
    "\n",
    "# Hyperparameters\n",
    "# Let's get rid of the dictionary and make these constant values instead. \n",
    "\"\"\"\n",
    "hparams = {\n",
    "    'EPOCHS': 20,\n",
    "    'LR': 5e-5,\n",
    "    'OPTIMIZER': 'Adam',\n",
    "    'BATCH_SIZE': 128\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "EPOCH_COuNT = 20\n",
    "LR = 5e-5\n",
    "OPTIMIZER = 'ADAM'\n",
    "BATCH_SIZE = 128\n",
    "LOG_PATH = './runs'\n",
    "\n",
    "# Mpve this code from the loader module. \n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "# Create our data location constants by using the Pathlib module \n",
    "# from Python\n",
    "TEST_DATA = pathlib.Path(f\"{DATA_DIR}/t10k-images-idx3-wbyte.gz\")\n",
    "TEST_LABELS = pathlib.Path(f\"{DATA_DIR}/t10k-labels-idx1-ubyte.gz\")\n",
    "TRAIN_DATA = pathlib.Path(f\"{DATA_DIR}/train-images-idx3-wbyte.gz\")\n",
    "TRAIN_LABELS = pathlib.Path(f\"{DATA_DIR}/train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Let's create a main method here. \n",
    "\n",
    "def main():\n",
    "    # Data\n",
    "    #Swap out dictionary lookups for constants. \n",
    "    # train_loader = get_train_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "    # test_loader = get_test_dataloader(batch_size=hparams.get('BATCH_SIZE'))\n",
    "    \n",
    "    train_loader = get_train_dataloader(batch_size = BATCH_SIZE)\n",
    "    test_loader = get_test_dataloader(batch_size = BATCH_SIZE\n",
    "\n",
    "    # Model and Optimizer\n",
    "    model = LinearNet()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=hparams.get('LR'))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR\n",
    "\n",
    "    # Here we can now create our runner objects. \n",
    "    test_runner = Runner(test_loader,model)\n",
    "    train_runner = Runner(train_loader, model, optimizer) # Remember that the train runner needs the optimizer. \n",
    "\n",
    "    # Experiment Trackers\n",
    "    # Note that the log_dir definition uses a string constant to specify the log directory. \n",
    "    # Let's use the LOG_PATH constant instead. \n",
    "    # log_dir = generate_tensorboard_experiment_directory(root='./runs')\n",
    "    log_dir = generate_tensorboard_experiment_directory(root=LOG_PATH)\n",
    "\n",
    "                                 \n",
    "    experiment = TensorboardExperiment(log_dir=log_dir)\n",
    "\n",
    "    # for epoch_id in range(hparams.get('EPOCHS')):\n",
    "    for epoch_id in range(EPOCH_COUNT):\n",
    "        run_epoch(test_runner, train_runner, experiment, epoch_id, EPOCH_COUNT)\n",
    "        \n",
    "       \n",
    "    experiment.flush()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our data loader.  We'll do some refacctoring here as well  Note that we can shorten and simplify the loops that load the data since most of the code is identical.  Ideally, the assert functions in the loader should get moved out to a unit testing framework such as pyunit  since we want to separate the responsibility of loading the data fron testing.  Here's the new loader.py module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ALLOWED_TYPES = {\n",
    "    \"UNSIGNED_BYTE\": b\"\\x08\",\n",
    "    \"SIGNED_BYTE\": b\"\\x09\",\n",
    "    \"SHORT\": b\"\\x0B\",\n",
    "    \"INT\": b\"\\x0C\",\n",
    "    \"SINGLE\": b\"\\x0D\",\n",
    "    \"DOUBLE\": b\"\\x0E\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_image_data(file_path: Path) -> np.ndarray:\n",
    "    with gzip.open(file_path, \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 3\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "        (num_rows,) = struct.unpack(\">I\", fp.read(4))\n",
    "        (num_cols,) = struct.unpack(\">I\", fp.read(4))\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images * num_rows * num_cols\n",
    "\n",
    "    data: np.ndarray = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    data = data.reshape((num_images, num_rows, num_cols))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_label_data(file_path: Path) -> np.ndarray:\n",
    "    with gzip.open(file_path, \"rb\") as fp:\n",
    "        _ = struct.unpack(\">H\", fp.read(2))  # dump padding bytes\n",
    "\n",
    "        (data_type,) = struct.unpack(\">c\", fp.read(1))\n",
    "        assert data_type == ALLOWED_TYPES[\"UNSIGNED_BYTE\"]\n",
    "\n",
    "        number_of_dimensions = ord(struct.unpack(\">c\", fp.read(1))[0])\n",
    "        assert number_of_dimensions == 1\n",
    "\n",
    "        (num_images,) = struct.unpack(\">I\", fp.read(4))\n",
    "\n",
    "        raw = fp.read()\n",
    "        assert len(raw) == num_images\n",
    "\n",
    "    data = np.frombuffer(raw, dtype=np.dtype(np.uint8).newbyteorder(\">\"))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a depedency in the dataset module.  We see that this modue is depedent on the old test and train load methods in the original version of the loader module.  This is a prime target for refactoring. \n",
    "\n",
    "We see in the data loader that we are specifically dependent on imports from the dataset module.  We also see that we have a test and a train loader method that are nearly identical.  So, we can combine them into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Bad!  This is a dependency.  \n",
    "from src.load_data import load_train_labels, load_train_data, load_test_data, load_test_labels\n",
    "\n",
    "\n",
    "class MNIST(Dataset):\n",
    "    idx: int  # requested data index\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "    TRAIN_MAX = 255.0\n",
    "    TRAIN_NORMALIZED_MEAN = 0.1306604762738429\n",
    "    TRAIN_NORMALIZED_STDEV = 0.3081078038564622\n",
    "\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        if len(data) != len(targets):\n",
    "            raise ValueError('data and targets must be the same length. '\n",
    "                             f'{len(data)} != {len(targets)}')\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.get_x(idx)\n",
    "        y = self.get_y(idx)\n",
    "        return x, y\n",
    "\n",
    "    def get_x(self, idx: int):\n",
    "        self.idx = idx\n",
    "        self.preprocess_x()\n",
    "        return self.x\n",
    "\n",
    "    def preprocess_x(self):\n",
    "        self.x = self.data[self.idx].copy().astype(np.float64)\n",
    "        self.x /= self.TRAIN_MAX\n",
    "        self.x -= self.TRAIN_NORMALIZED_MEAN\n",
    "        self.x /= self.TRAIN_NORMALIZED_STDEV\n",
    "        self.x = self.x.astype(np.float32)\n",
    "        self.x = torch.from_numpy(self.x)\n",
    "        self.x = self.x.unsqueeze(0)\n",
    "\n",
    "    def get_y(self, idx: int):\n",
    "        self.idx = idx\n",
    "        self.preprocess_y()\n",
    "        return self.y\n",
    "\n",
    "    def preprocess_y(self):\n",
    "        self.y = self.targets[self.idx]\n",
    "        self.y = torch.tensor(self.y, dtype=torch.long)\n",
    "\n",
    "\n",
    "# The next two methods are nearly identical.  let's combine them. \n",
    "def get_train_dataloader(batch_size: int) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        dataset=MNIST(load_train_data(), load_train_labels()),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "def get_test_dataloader(batch_size: int) -> DataLoader:\n",
    "    return DataLoader(\n",
    "        dataset=MNIST(load_test_data(), load_test_labels()),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our new dataset module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from ds.load_data import load_image_data, load_label_data\n",
    "\n",
    "\n",
    "class MNIST(Dataset[Any]):\n",
    "    idx: int  # requested data index\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "    TRAIN_MAX = 255.0\n",
    "    TRAIN_NORMALIZED_MEAN = 0.1306604762738429\n",
    "    TRAIN_NORMALIZED_STDEV = 0.3081078038564622\n",
    "\n",
    "    def __init__(self, data: np.ndarray, targets: np.ndarray):\n",
    "        if len(data) != len(targets):\n",
    "            raise ValueError(\n",
    "                \"data and targets must be the same length. \"\n",
    "                f\"{len(data)} != {len(targets)}\"\n",
    "            )\n",
    "\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.get_x(idx)\n",
    "        y = self.get_y(idx)\n",
    "        return x, y\n",
    "\n",
    "    def get_x(self, idx: int):\n",
    "        self.idx = idx\n",
    "        self.preprocess_x()\n",
    "        return self.x\n",
    "\n",
    "    def preprocess_x(self):\n",
    "        self.x = self.data[self.idx].copy().astype(np.float64)\n",
    "        self.x /= self.TRAIN_MAX\n",
    "        self.x -= self.TRAIN_NORMALIZED_MEAN\n",
    "        self.x /= self.TRAIN_NORMALIZED_STDEV\n",
    "        self.x = self.x.astype(np.float32)\n",
    "        self.x = torch.from_numpy(self.x)\n",
    "        self.x = self.x.unsqueeze(0)\n",
    "\n",
    "    def get_y(self, idx: int):\n",
    "        self.idx = idx\n",
    "        self.preprocess_y()\n",
    "        return self.y\n",
    "\n",
    "    def preprocess_y(self):\n",
    "        self.y = self.targets[self.idx]\n",
    "        self.y = torch.tensor(self.y, dtype=torch.long)\n",
    "\n",
    "# Combined methods ito a create_dataloader.  Note that we set shuffle to True i the train_dataloader\n",
    "# method, but False in the test_dataloader method. \n",
    "\n",
    "def create_dataloader(\n",
    "    batch_size: int, data_path: Path, label_path: Path, shuffle: bool = True\n",
    ") -> DataLoader[Any]:\n",
    "    data = load_image_data(data_path)\n",
    "    label_data = load_label_data(label_path)\n",
    "    return DataLoader(\n",
    "        dataset=MNIST(data, label_data),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that we need to change a line in the main.py file so that instead of calling a train and test loader, we just call the create_dataloader function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    " #train_loader = get_train_dataloader(batch_size = BATCH_SIZE)\n",
    "# test_loader = get_test_dataloader(batch_size = BATCH_SIZE)\n",
    "train_loader = create_dataloader(BATCH_SIZE, TRAIN_DATA, TRAIN_LABELS)\n",
    "test_loader = create_dataloader(BATCH_SIZE,TEST_DATA, TEST_LABELS)\n",
    "...\n",
    "                                      \n",
    "                                      \n",
    "                                      ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
